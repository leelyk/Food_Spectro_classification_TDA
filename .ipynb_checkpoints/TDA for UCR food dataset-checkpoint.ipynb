{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "fe9b3020-ba86-4cfb-960c-36c853c3025f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from gtda.time_series import SingleTakensEmbedding     # Implement the sliding window and the time delay embedding method.\n",
    "\n",
    "from ripser import ripser                              # The Ripser package is used to compute Persistent Homology.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import itertools                                       # This package is primarily used to handle iterable objects.\n",
    "                                                                                 # （ 我們主要用來將二維陣列合併成一維為陣列 ）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6521540-d315-4ad1-a2b2-8ba5771592ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## （1）Sliding Window Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "4046ec11-713f-47ae-b5c7-14f3ed1e436e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sliding shift = 1 \n",
    "\n",
    "# signal is a 1-dimensional array of time series data.\n",
    "\n",
    "def sliding_windows(signal, window_size):\n",
    "    \n",
    "    window = SingleTakensEmbedding(parameters_type = \"fixed\", dimension = window_size)\n",
    "    \n",
    "    sliding_windows = window.fit_transform(signal)\n",
    "    \n",
    "    return sliding_windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c899f542-d82f-4258-997d-cefea21bcb88",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## （2）TDA and Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fa6210-9672-40c5-87e5-331cece65b0d",
   "metadata": {},
   "source": [
    "### <font color = SteelBlue >Takens Embedding and Persistent Homology（時間延遲嵌入與持久同調）</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "657ebebd-0a80-4af8-b6a0-8206a78376d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#                                                                                                                                   #\n",
    "# The takens_embedding method transforms a time series signal into a point cloud.                                                   #\n",
    "#                                                                                                                                   #\n",
    "# The persistent_homology method is used to compute Persistent Homology and transforms a point cloudd into a persistence diagram.   #\n",
    "#                                                                                                                                   #\n",
    "#####################################################################################################################################\n",
    "\n",
    "def takens_embedding(signal, embedding_dimension):\n",
    "    \n",
    "    embedder = SingleTakensEmbedding(parameters_type = \"fixed\", dimension = embedding_dimension)\n",
    "    \n",
    "    point_cloud = embedder.fit_transform(signal)\n",
    "    \n",
    "    return point_cloud\n",
    "\n",
    "def persistent_homology(point_cloud):\n",
    "    \n",
    "    persistence_diagram = ripser(point_cloud)['dgms']\n",
    "    \n",
    "    return persistence_diagram "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f9b6e8-ee21-4c1d-b1b3-09be8bcc34ff",
   "metadata": {},
   "source": [
    "### <font color = SteelBlue >Feature Extraction 1：get_diagram_lives（計算持續圖所有點的壽命）</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "d68fed22-1259-4163-ac5d-17c6844e6e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the life(death - birth) of all points in the persistence diagram. \n",
    "\n",
    "def get_diagram_lives(diagram):\n",
    "    \n",
    "    return diagram[:,1] - diagram[:,0] if len(diagram) > 0 else np.array([0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc92872-b8ee-4eca-b648-e1a80184a81c",
   "metadata": {},
   "source": [
    "### <font color = SteelBlue >Feature Extraction 2：get_diagram_entropy（計算持續傷）</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "5c24ea70-4ec1-4f0b-874b-f0b2c19a86f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the persistence entropy.\n",
    "\n",
    "def get_diagram_entropy(lives):\n",
    "    \n",
    "    if max(abs(lives)) == 0:\n",
    "        \n",
    "        return 0.\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        normalized_lives = lives / sum(lives)\n",
    "        \n",
    "        return sum(-normalized_lives * np.log(normalized_lives))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e05d98-a380-4c83-8ed1-3a69902a2196",
   "metadata": {},
   "source": [
    "### <font color = SteelBlue >Feature Extraction 3：get_diagram_features（ 將全部三個特徵整理在一組陣列）</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "41025d35-2d94-42e8-96b2-c86d233202f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The get_diagram_features method organizes all three features into a single array.\n",
    "\n",
    "def get_diagram_features(diagram):\n",
    "    \n",
    "    diagram = diagram[~np.any(np.isinf(diagram), axis = 1)]   # Remove points with inf values from the persistence diagram (for H0).\n",
    "    \n",
    "    lives = get_diagram_lives(diagram)\n",
    "    \n",
    "    bottleneck_distance = max(lives) / np.sqrt(2)        # feature 1（ bottleneck distance ）\n",
    "    \n",
    "    wasserstein_distance = sum(lives) / np.sqrt(2)       # feature 2（ wasserstein distance ）\n",
    "    \n",
    "    persistence_entropy = get_diagram_entropy(lives)     # feature 3（ persistence entropy ）\n",
    "    \n",
    "    result = [bottleneck_distance, wasserstein_distance, persistence_entropy] \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e8c749-a09b-44c2-9c37-1c402cddef1b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## （3）UCR Time Series Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "3b1f0ed8-0640-43c0-b05e-4b2d697558aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method splits the dataset into training and testing sets and converts them into DataFrames.\n",
    "\n",
    "def beef_data():\n",
    "    \n",
    "    # Read a txt file and convert it into a DataFrame.\n",
    "    \n",
    "    data_train = pd.read_csv('UCR_datasets/Beef_TRAIN.txt', sep = '\\s+', header = None)\n",
    "    \n",
    "    data_test = pd.read_csv('UCR_datasets/Beef_TEST.txt', sep = '\\s+', header = None)\n",
    "    \n",
    "    # Separate the targets from the time series signal.\n",
    "    \n",
    "    train_targets = data_train.iloc[:, 0].astype(int)  \n",
    "    \n",
    "    train_signals = data_train.iloc[:, 1:].astype(float)  \n",
    "\n",
    "    test_targets = data_test.iloc[:, 0].astype(int)  \n",
    "    \n",
    "    test_signals = data_test.iloc[:, 1:].astype(float)  \n",
    "\n",
    "    # Convert the time series data in each row into a list format.\n",
    "    \n",
    "    train_signalss = []\n",
    "    \n",
    "    test_signalss = []\n",
    "    \n",
    "    train_signals_list = train_signals.values.tolist()\n",
    "    \n",
    "    for row in train_signals_list:\n",
    "        \n",
    "        train_signalss.append(row)\n",
    "    \n",
    "    test_signals_list = test_signals.values.tolist()\n",
    "    \n",
    "    for row in test_signals_list:\n",
    "        \n",
    "        test_signalss.append(row)\n",
    "\n",
    "    # Create a new DataFrame where the signal is in list format.\n",
    "    \n",
    "    df_train = pd.DataFrame({\n",
    "        \n",
    "        'signal': train_signalss,\n",
    "        \n",
    "        'target': train_targets\n",
    "        \n",
    "    })\n",
    "\n",
    "    df_test = pd.DataFrame({\n",
    "        \n",
    "        'signal': test_signalss,\n",
    "        \n",
    "        'target': test_targets\n",
    "        \n",
    "    })\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "8d9c0bdf-948c-4d59-b37f-60c4f27a88d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method splits the dataset into training and testing sets and converts them into DataFrames.\n",
    "\n",
    "def coffee_data():\n",
    "    \n",
    "    # Read a txt file and convert it into a DataFrame.\n",
    "    \n",
    "    data_train = pd.read_csv('UCR_datasets/Coffee_TRAIN.txt', sep='\\s+', header=None)\n",
    "    \n",
    "    data_test = pd.read_csv('UCR_datasets/Coffee_TEST.txt', sep='\\s+', header=None)\n",
    "    \n",
    "    # Separate the targets from the time series signal.\n",
    "    \n",
    "    train_targets = data_train.iloc[:, 0].astype(int)  \n",
    "    \n",
    "    train_signals = data_train.iloc[:, 1:].astype(float)  \n",
    "\n",
    "    test_targets = data_test.iloc[:, 0].astype(int)  \n",
    "    \n",
    "    test_signals = data_test.iloc[:, 1:].astype(float)  \n",
    "\n",
    "    # Convert the time series data in each row into a list format.\n",
    "    \n",
    "    train_signalss = []\n",
    "    \n",
    "    test_signalss = []\n",
    "    \n",
    "    train_signals_list = train_signals.values.tolist()\n",
    "    \n",
    "    for row in train_signals_list:\n",
    "        \n",
    "        train_signalss.append(row)\n",
    "    \n",
    "    test_signals_list = test_signals.values.tolist()\n",
    "    \n",
    "    for row in test_signals_list:\n",
    "        \n",
    "        test_signalss.append(row)\n",
    "\n",
    "    # Create a new DataFrame where the signal is in list format.\n",
    "    \n",
    "    df_train = pd.DataFrame({\n",
    "        \n",
    "        'signal': train_signalss,\n",
    "        \n",
    "        'target': train_targets\n",
    "        \n",
    "    })\n",
    "\n",
    "    df_test = pd.DataFrame({\n",
    "        \n",
    "        'signal': test_signalss,\n",
    "        \n",
    "        'target': test_targets\n",
    "        \n",
    "    })\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "1fcbb969-020f-4a7f-9ad5-ed87439da4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method splits the dataset into training and testing sets and converts them into DataFrames.\n",
    "\n",
    "def ham_data():\n",
    "    \n",
    "    # Read a txt file and convert it into a DataFrame.\n",
    "    \n",
    "    data_train = pd.read_csv('UCR_datasets/Ham_TRAIN.txt', sep='\\s+', header=None)\n",
    "    \n",
    "    data_test = pd.read_csv('UCR_datasets/Ham_TEST.txt', sep='\\s+', header=None)\n",
    "    \n",
    "    # Separate the targets from the time series signal.\n",
    "    \n",
    "    train_targets = data_train.iloc[:, 0].astype(int)  \n",
    "    \n",
    "    train_signals = data_train.iloc[:, 1:].astype(float)  \n",
    "\n",
    "    test_targets = data_test.iloc[:, 0].astype(int)  \n",
    "    \n",
    "    test_signals = data_test.iloc[:, 1:].astype(float)  \n",
    "\n",
    "    # Convert the time series data in each row into a list format.\n",
    "    \n",
    "    train_signalss = []\n",
    "    \n",
    "    test_signalss = []\n",
    "    \n",
    "    train_signals_list = train_signals.values.tolist()\n",
    "    \n",
    "    for row in train_signals_list:\n",
    "        \n",
    "        train_signalss.append(row)\n",
    "    \n",
    "    test_signals_list = test_signals.values.tolist()\n",
    "    \n",
    "    for row in test_signals_list:\n",
    "        \n",
    "        test_signalss.append(row)\n",
    "\n",
    "    # Create a new DataFrame where the signal is in list format.\n",
    "    \n",
    "    df_train = pd.DataFrame({\n",
    "        \n",
    "        'signal': train_signalss,\n",
    "        \n",
    "        'target': train_targets\n",
    "        \n",
    "    })\n",
    "\n",
    "    df_test = pd.DataFrame({\n",
    "        \n",
    "        'signal': test_signalss,\n",
    "        \n",
    "        'target': test_targets\n",
    "        \n",
    "    })\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "9561c4c7-d93b-433e-a0ce-6d5a49e024a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method splits the dataset into training and testing sets and converts them into DataFrames.\n",
    "\n",
    "def meat_data():\n",
    "    \n",
    "    # Read a txt file and convert it into a DataFrame.\n",
    "    \n",
    "    data_train = pd.read_csv('UCR_datasets/Meat_TRAIN.txt', sep='\\s+', header=None)\n",
    "    \n",
    "    data_test = pd.read_csv('UCR_datasets/Meat_TEST.txt', sep='\\s+', header=None)\n",
    "    \n",
    "    # Separate the targets from the time series signal.\n",
    "    \n",
    "    train_targets = data_train.iloc[:, 0].astype(int)  \n",
    "    \n",
    "    train_signals = data_train.iloc[:, 1:].astype(float)  \n",
    "\n",
    "    test_targets = data_test.iloc[:, 0].astype(int)  \n",
    "    \n",
    "    test_signals = data_test.iloc[:, 1:].astype(float)  \n",
    "\n",
    "    # Convert the time series data in each row into a list format.\n",
    "    \n",
    "    train_signalss = []\n",
    "    \n",
    "    test_signalss = []\n",
    "    \n",
    "    train_signals_list = train_signals.values.tolist()\n",
    "    \n",
    "    for row in train_signals_list:\n",
    "        \n",
    "        train_signalss.append(row)\n",
    "    \n",
    "    test_signals_list = test_signals.values.tolist()\n",
    "    \n",
    "    for row in test_signals_list:\n",
    "        \n",
    "        test_signalss.append(row)\n",
    "\n",
    "    # Create a new DataFrame where the signal is in list format.\n",
    "    \n",
    "    df_train = pd.DataFrame({\n",
    "        \n",
    "        'signal': train_signalss,\n",
    "        \n",
    "        'target': train_targets\n",
    "        \n",
    "    })\n",
    "\n",
    "    df_test = pd.DataFrame({\n",
    "        \n",
    "        'signal': test_signalss,\n",
    "        \n",
    "        'target': test_targets\n",
    "        \n",
    "    })\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "4b3c33a1-55ee-4b84-b461-48a6e6d2f0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method splits the dataset into training and testing sets and converts them into DataFrames.\n",
    "\n",
    "def oliveoil_data():\n",
    "    \n",
    "    # Read a txt file and convert it into a DataFrame.\n",
    "    \n",
    "    data_train = pd.read_csv('UCR_datasets/OliveOil_TRAIN.txt', sep='\\s+', header=None)\n",
    "    \n",
    "    data_test = pd.read_csv('UCR_datasets/OliveOil_TEST.txt', sep='\\s+', header=None)\n",
    "    \n",
    "    # Separate the targets from the time series signal.\n",
    "    \n",
    "    train_targets = data_train.iloc[:, 0].astype(int)  \n",
    "    \n",
    "    train_signals = data_train.iloc[:, 1:].astype(float)  \n",
    "\n",
    "    test_targets = data_test.iloc[:, 0].astype(int)  \n",
    "    \n",
    "    test_signals = data_test.iloc[:, 1:].astype(float)  \n",
    "\n",
    "    # Convert the time series data in each row into a list format.\n",
    "    \n",
    "    train_signalss = []\n",
    "    \n",
    "    test_signalss = []\n",
    "    \n",
    "    train_signals_list = train_signals.values.tolist()\n",
    "    \n",
    "    for row in train_signals_list:\n",
    "        \n",
    "        train_signalss.append(row)\n",
    "    \n",
    "    test_signals_list = test_signals.values.tolist()\n",
    "    \n",
    "    for row in test_signals_list:\n",
    "        \n",
    "        test_signalss.append(row)\n",
    "\n",
    "    # Create a new DataFrame where the signal is in list format.\n",
    "    \n",
    "    df_train = pd.DataFrame({\n",
    "        \n",
    "        'signal': train_signalss,\n",
    "        \n",
    "        'target': train_targets\n",
    "        \n",
    "    })\n",
    "\n",
    "    df_test = pd.DataFrame({\n",
    "        \n",
    "        'signal': test_signalss,\n",
    "        \n",
    "        'target': test_targets\n",
    "        \n",
    "    })\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "85f4a5e2-b72f-40d9-9917-7dc72a980598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method splits the dataset into training and testing sets and converts them into DataFrames.\n",
    "\n",
    "def strawberry_data():\n",
    "    \n",
    "    # Read a txt file and convert it into a DataFrame.\n",
    "    \n",
    "    data_train = pd.read_csv('UCR_datasets/Strawberry_TRAIN.txt', sep='\\s+', header=None)\n",
    "    \n",
    "    data_test = pd.read_csv('UCR_datasets/Strawberry_TEST.txt', sep='\\s+', header=None)\n",
    "    \n",
    "    # Separate the targets from the time series signal.\n",
    "    \n",
    "    train_targets = data_train.iloc[:, 0].astype(int)  \n",
    "    \n",
    "    train_signals = data_train.iloc[:, 1:].astype(float)  \n",
    "\n",
    "    test_targets = data_test.iloc[:, 0].astype(int)  \n",
    "    \n",
    "    test_signals = data_test.iloc[:, 1:].astype(float)  \n",
    "\n",
    "    # Convert the time series data in each row into a list format.\n",
    "    \n",
    "    train_signalss = []\n",
    "    \n",
    "    test_signalss = []\n",
    "    \n",
    "    train_signals_list = train_signals.values.tolist()\n",
    "    \n",
    "    for row in train_signals_list:\n",
    "        \n",
    "        train_signalss.append(row)\n",
    "    \n",
    "    test_signals_list = test_signals.values.tolist()\n",
    "    \n",
    "    for row in test_signals_list:\n",
    "        \n",
    "        test_signalss.append(row)\n",
    "\n",
    "    # Create a new DataFrame where the signal is in list format.\n",
    "    \n",
    "    df_train = pd.DataFrame({\n",
    "        \n",
    "        'signal': train_signalss,\n",
    "        \n",
    "        'target': train_targets\n",
    "        \n",
    "    })\n",
    "\n",
    "    df_test = pd.DataFrame({\n",
    "        \n",
    "        'signal': test_signalss,\n",
    "        \n",
    "        'target': test_targets\n",
    "        \n",
    "    })\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "3abfd7ba-dfee-4979-a409-34d4388da64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method splits the dataset into training and testing sets and converts them into DataFrames.\n",
    "\n",
    "def wine_data():\n",
    "    \n",
    "    # Read a txt file and convert it into a DataFrame.\n",
    "    \n",
    "    data_train = pd.read_csv('UCR_datasets/Wine_TRAIN.txt', sep='\\s+', header=None)\n",
    "    \n",
    "    data_test = pd.read_csv('UCR_datasets/Wine_TEST.txt', sep='\\s+', header=None)\n",
    "    \n",
    "    # Separate the targets from the time series signal.\n",
    "    \n",
    "    train_targets = data_train.iloc[:, 0].astype(int)  \n",
    "    \n",
    "    train_signals = data_train.iloc[:, 1:].astype(float)  \n",
    "\n",
    "    test_targets = data_test.iloc[:, 0].astype(int)  \n",
    "    \n",
    "    test_signals = data_test.iloc[:, 1:].astype(float)  \n",
    "\n",
    "    # Convert the time series data in each row into a list format.\n",
    "    \n",
    "    train_signalss = []\n",
    "    \n",
    "    test_signalss = []\n",
    "    \n",
    "    train_signals_list = train_signals.values.tolist()\n",
    "    \n",
    "    for row in train_signals_list:\n",
    "        \n",
    "        train_signalss.append(row)\n",
    "    \n",
    "    test_signals_list = test_signals.values.tolist()\n",
    "    \n",
    "    for row in test_signals_list:\n",
    "        \n",
    "        test_signalss.append(row)\n",
    "\n",
    "    # Create a new DataFrame where the signal is in list format.\n",
    "    \n",
    "    df_train = pd.DataFrame({\n",
    "        \n",
    "        'signal': train_signalss,\n",
    "        \n",
    "        'target': train_targets\n",
    "        \n",
    "    })\n",
    "\n",
    "    df_test = pd.DataFrame({\n",
    "        \n",
    "        'signal': test_signalss,\n",
    "        \n",
    "        'target': test_targets\n",
    "        \n",
    "    })\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d56a61-27b9-4e99-a02b-0b0a9436ddd2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## （4）Machine Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "66c86ec6-ff31-4eb5-9ede-2a12e7465fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "332239a9-c278-4a99-b347-4841b8c4ee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Beef\n",
    "\n",
    "def beef_classification_model(train_features, train_target, test_features, test_target):\n",
    "    \n",
    "    X_train = train_features\n",
    "    \n",
    "    y_train = train_target\n",
    "    \n",
    "    X_test = test_features\n",
    "    \n",
    "    y_test = test_target\n",
    "\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), ('bc', BaggingClassifier(estimator = LogisticRegression(max_iter = 3000, C = 0.01)))])\n",
    "    \n",
    "    param_grid = {\n",
    "        \n",
    "    'bc__n_estimators': [50],\n",
    "        \n",
    "    'bc__random_state': [14]\n",
    "        \n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(pipe, param_grid, cv = 5, scoring = 'accuracy', verbose = 1, n_jobs = -1)\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"最佳參數: {grid_search.best_params_}\")\n",
    "    \n",
    "    print(f\"最佳交叉驗證準確率: {grid_search.best_score_}\")\n",
    "    \n",
    "    y_pred = grid_search.predict(X_test)\n",
    "\n",
    "    bc_accuracy = round(100 * (accuracy_score(y_test, y_pred)), 3)\n",
    "    \n",
    "    print('bc Test accuracy:', accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    return bc_accuracy\n",
    "\n",
    "# For Coffee\n",
    "\n",
    "def coffee_classification_model(train_features, train_target, test_features, test_target):\n",
    "    \n",
    "    X_train = train_features\n",
    "    \n",
    "    y_train = train_target\n",
    "    \n",
    "    X_test = test_features\n",
    "    \n",
    "    y_test = test_target\n",
    "\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), ('bc', BaggingClassifier(estimator = LogisticRegression(max_iter = 1800)))])\n",
    "    \n",
    "    param_grid = {\n",
    "        \n",
    "    'bc__n_estimators': [100, 200],\n",
    "        \n",
    "    'bc__max_samples': [1.0, 0.8, 0.6],\n",
    "        \n",
    "    'bc__random_state': [14]\n",
    "        \n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(pipe, param_grid, cv = 3, scoring = 'accuracy', verbose = 1, n_jobs = -1)\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"最佳參數: {grid_search.best_params_}\")\n",
    "    \n",
    "    print(f\"最佳交叉驗證準確率: {grid_search.best_score_}\")\n",
    "    \n",
    "    y_pred = grid_search.predict(X_test)\n",
    "\n",
    "    bc_accuracy = round(100 * (accuracy_score(y_test, y_pred)), 3)\n",
    "    \n",
    "    print('bc Test accuracy:', accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    return bc_accuracy\n",
    "\n",
    "# For Ham\n",
    "\n",
    "def ham_classification_model(train_features, train_target, test_features, test_target):\n",
    "    \n",
    "    X_train = train_features\n",
    "    \n",
    "    y_train = train_target\n",
    "    \n",
    "    X_test = test_features\n",
    "    \n",
    "    y_test = test_target\n",
    "\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), ('bc', BaggingClassifier(estimator = LogisticRegression(max_iter = 2000, solver = 'lbfgs')))])\n",
    "    \n",
    "    param_grid = {\n",
    "        \n",
    "    'bc__n_estimators': [10, 20, 30, 50, 100],\n",
    "        \n",
    "    'bc__max_samples': [1.0, 0.8, 0.6],\n",
    "        \n",
    "    'bc__max_features': [1.0, 0.8],\n",
    "        \n",
    "    'bc__random_state': [45]\n",
    "        \n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(pipe, param_grid, cv = 3, scoring = 'accuracy', verbose = 1, n_jobs = -1)\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"最佳參數: {grid_search.best_params_}\")\n",
    "    \n",
    "    print(f\"最佳交叉驗證準確率: {grid_search.best_score_}\")\n",
    "    \n",
    "    y_pred = grid_search.predict(X_test)\n",
    "\n",
    "    bc_accuracy = round(100 * (accuracy_score(y_test, y_pred)), 3)\n",
    "    \n",
    "    print('bc Test accuracy:', accuracy_score(y_test, y_pred))\n",
    "\n",
    "    return bc_accuracy\n",
    "\n",
    "# For Meat\n",
    "\n",
    "def meat_classification_model(train_features, train_target, test_features, test_target):\n",
    "    \n",
    "    X_train = train_features\n",
    "    \n",
    "    y_train = train_target\n",
    "    \n",
    "    X_test = test_features\n",
    "    \n",
    "    y_test = test_target\n",
    "\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), ('bc', BaggingClassifier(estimator = LogisticRegression(max_iter = 2000, C = 0.01, solver = 'sag')))])\n",
    "    \n",
    "    param_grid = {\n",
    "        \n",
    "    'bc__n_estimators': [10, 20, 30, 50, 100],\n",
    "        \n",
    "    'bc__max_samples': [1.0, 0.8, 0.6],\n",
    "        \n",
    "    'bc__max_features': [1.0, 0.8],\n",
    "        \n",
    "    'bc__random_state': [42]\n",
    "        \n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(pipe, param_grid, cv = 3, scoring = 'accuracy', verbose = 1, n_jobs = -1)\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"最佳參數: {grid_search.best_params_}\")\n",
    "    \n",
    "    print(f\"最佳交叉驗證準確率: {grid_search.best_score_}\")\n",
    "    \n",
    "    y_pred = grid_search.predict(X_test)\n",
    "\n",
    "    bc_accuracy = round(100 * (accuracy_score(y_test, y_pred)), 3)\n",
    "    \n",
    "    print('bc Test accuracy:', accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    return bc_accuracy\n",
    "\n",
    "# For Olive oil\n",
    "\n",
    "def oliveoil_classification_model(train_features, train_target, test_features, test_target):\n",
    "    \n",
    "    X_train = train_features\n",
    "    \n",
    "    y_train = train_target\n",
    "    \n",
    "    X_test = test_features\n",
    "    \n",
    "    y_test = test_target\n",
    "\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), ('bc', BaggingClassifier(estimator = LogisticRegression(max_iter = 3500, C = 0.01, solver = 'sag')))])\n",
    "    \n",
    "    param_grid = {\n",
    "        \n",
    "    'bc__n_estimators': [10, 20, 30, 50, 100],\n",
    "        \n",
    "    'bc__max_samples': [1.0, 0.8, 0.6],\n",
    "        \n",
    "    'bc__max_features': [1.0, 0.8],\n",
    "        \n",
    "    'bc__random_state': [14]\n",
    "        \n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(pipe, param_grid, cv = 3, scoring = 'accuracy', verbose = 1, n_jobs = -1)\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"最佳參數: {grid_search.best_params_}\")\n",
    "    \n",
    "    print(f\"最佳交叉驗證準確率: {grid_search.best_score_}\")\n",
    "    \n",
    "    y_pred = grid_search.predict(X_test)\n",
    "\n",
    "    bc_accuracy = round(100 * (accuracy_score(y_test, y_pred)), 3)\n",
    "    \n",
    "    print('bc Test accuracy:', accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    return bc_accuracy\n",
    "    \n",
    "# For Strawberry \n",
    "\n",
    "def strawberry_classification_model(train_features, train_target, test_features, test_target):\n",
    "    \n",
    "    X_train = train_features\n",
    "    \n",
    "    y_train = train_target\n",
    "    \n",
    "    X_test = test_features\n",
    "    \n",
    "    y_test = test_target\n",
    "\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), ('bc', BaggingClassifier(estimator = KNeighborsClassifier(n_neighbors = 1, algorithm = 'ball_tree', leaf_size = 20, p = 1)))])\n",
    "    \n",
    "    param_grid = {\n",
    "        \n",
    "    'bc__n_estimators': [10, 20, 30, 50, 100],\n",
    "        \n",
    "    'bc__max_samples': [1.0, 0.8, 0.6],\n",
    "        \n",
    "    'bc__max_features': [1.0, 0.8],\n",
    "        \n",
    "    'bc__random_state': [14]\n",
    "        \n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(pipe, param_grid, cv = 3, scoring = 'accuracy', verbose = 1, n_jobs = -1)\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"最佳參數: {grid_search.best_params_}\")\n",
    "    \n",
    "    print(f\"最佳交叉驗證準確率: {grid_search.best_score_}\")\n",
    "    \n",
    "    y_pred = grid_search.predict(X_test)\n",
    "\n",
    "    bc_accuracy = round(100 * (accuracy_score(y_test, y_pred)), 3)\n",
    "    \n",
    "    print('bc Test accuracy:', accuracy_score(y_test, y_pred))\n",
    "\n",
    "    return bc_accuracy\n",
    "\n",
    "# For Wine\n",
    "\n",
    "def wine_classification_model(train_features, train_target, test_features, test_target):\n",
    "    \n",
    "    X_train = train_features\n",
    "    \n",
    "    y_train = train_target\n",
    "    \n",
    "    X_test = test_features\n",
    "    \n",
    "    y_test = test_target\n",
    "\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), ('bc', BaggingClassifier(estimator = LogisticRegression(max_iter = 1800)))])\n",
    "    \n",
    "    param_grid = {\n",
    "        \n",
    "    'bc__n_estimators': [10, 30, 50, 100],\n",
    "        \n",
    "    'bc__max_samples': [1.0, 0.8, 0.6],\n",
    "        \n",
    "    'bc__random_state': [3]\n",
    "        \n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(pipe, param_grid, cv = 3, scoring = 'accuracy', verbose = 1, n_jobs = -1)\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"最佳參數: {grid_search.best_params_}\")\n",
    "    \n",
    "    print(f\"最佳交叉驗證準確率: {grid_search.best_score_}\")\n",
    "    \n",
    "    y_pred = grid_search.predict(X_test)\n",
    "\n",
    "    bc_accuracy = round(100 * (accuracy_score(y_test, y_pred)), 3)\n",
    "    \n",
    "    print('bc Test accuracy:', accuracy_score(y_test, y_pred))  \n",
    "\n",
    "    return bc_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef42568-4be7-40bd-bfbf-5066d2478cf7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## （5）Convert Series to DataFrame："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "f0621691-dd85-4067-9171-003e5ae8c5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pd.Series([[1,2,3],[4,5,6]]) to pd.DataFrame([[1,2,3],[4,5,6]])\n",
    "\n",
    "def pd_series_to_dataframe(series):\n",
    "    \n",
    "    return pd.DataFrame.from_dict(dict(zip(series.index, series.values))).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce2ade5-1019-4f49-a2df-451eec39223f",
   "metadata": {},
   "source": [
    "## （6）Main Experiment（研究主體）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "22a88260-cce5-41fc-a483-7c5f79b7445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# beef main experiment\n",
    "\n",
    "def beef_experiment(embedding_dimension = 2):\n",
    "\n",
    "    all_classification_result = []   # Store all classification results.\n",
    "    \n",
    "    # Step 1. Call the beef_data() method to obtain the beef dataset and convert it into Series format.\n",
    "    \n",
    "    train_data, test_data = beef_data()\n",
    "\n",
    "    train_data_series = pd.Series(train_data['signal'].iloc[:])\n",
    "    \n",
    "    test_data_series = pd.Series(test_data['signal'].iloc[:])\n",
    "\n",
    "    window_size = [5, 10, 45, 50, 95, 100]        # (5, 10) = small size，(45, 50) = medium size，(95, 100) = large size\n",
    "\n",
    "    # Step 2. Apply a sliding window method to all time series in the dataset.\n",
    "    \n",
    "    for size in window_size:\n",
    "        \n",
    "        train_sliding_windows = train_data_series.map(lambda x: sliding_windows(x, size))\n",
    "        \n",
    "        test_sliding_windows = test_data_series.map(lambda x: sliding_windows(x, size))\n",
    "\n",
    "        # Step 3. Apply time-delay embedding method to each window and obtain a 2-dimensional point cloud.\n",
    "        \n",
    "        train_point_cloud = train_sliding_windows.map(lambda x: [takens_embedding(row, embedding_dimension) for row in x])\n",
    "        \n",
    "        test_point_cloud = test_sliding_windows.map(lambda x: [takens_embedding(row, embedding_dimension) for row in x])\n",
    "\n",
    "        # Step 4. Convert each point cloud into a persistence diagram.\n",
    "        \n",
    "        train_persistence_diagrams = train_point_cloud.map(lambda x: [persistent_homology(row) for row in x])\n",
    "        \n",
    "        test_persistence_diagrams = test_point_cloud.map(lambda x: [persistent_homology(row) for row in x])\n",
    "\n",
    "        # Step 5. Separate the H0 and H1 structures in the persistence diagram.\n",
    "        \n",
    "        train_H0 = train_persistence_diagrams.map(lambda x: [row[0] for row in x])\n",
    "        \n",
    "        train_H1 = train_persistence_diagrams.map(lambda x: [row[1] for row in x])\n",
    "\n",
    "        test_H0 = test_persistence_diagrams.map(lambda x: [row[0] for row in x])\n",
    "        \n",
    "        test_H1 = test_persistence_diagrams.map(lambda x: [row[1] for row in x])\n",
    "\n",
    "        # Step 6. Feature Extraction !\n",
    "\n",
    "        features_train_H0 = train_H0.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "        \n",
    "        features_train_H1 = train_H1.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "\n",
    "        features_test_H0 = test_H0.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "        \n",
    "        features_test_H1 = test_H1.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "\n",
    "        # Step 7. Use the itertools package to merge the features of each window into a 1-dimensional array.\n",
    "        \n",
    "        features_train_H0 = features_train_H0.map(lambda x: list(itertools.chain(*x)))\n",
    "        \n",
    "        features_train_H1 = features_train_H1.map(lambda x: list(itertools.chain(*x)))\n",
    "\n",
    "        features_test_H0 = features_test_H0.map(lambda x: list(itertools.chain(*x)))\n",
    "        \n",
    "        features_test_H1 = features_test_H1.map(lambda x: list(itertools.chain(*x)))\n",
    "\n",
    "        # Step 8. Convert all features of H0 and H1 into a DataFrame format, and merge them.\n",
    "        \n",
    "        features_train_H0 = pd_series_to_dataframe(features_train_H0)\n",
    "        \n",
    "        features_train_H1 = pd_series_to_dataframe(features_train_H1)\n",
    "\n",
    "        features_test_H0 = pd_series_to_dataframe(features_test_H0)\n",
    "        \n",
    "        features_test_H1 = pd_series_to_dataframe(features_test_H1)\n",
    "\n",
    "        features_train = pd.concat([features_train_H0, features_train_H1], axis=1)   #（主要解決兩個 Data Frame 合併後，列名重複問題！）\n",
    "        \n",
    "        features_train.columns = np.arange(features_train.shape[1])\n",
    "        \n",
    "        features_test = pd.concat([features_test_H0, features_test_H1], axis=1)\n",
    "        \n",
    "        features_test.columns = np.arange(features_test.shape[1])\n",
    "\n",
    "        # Step 9. Convert all features in each row into a list format.\n",
    "        \n",
    "        train_features = []\n",
    "        \n",
    "        features_train = features_train.values.tolist()\n",
    "        \n",
    "        for row in features_train:\n",
    "            \n",
    "            train_features.append(row)\n",
    "\n",
    "        test_features = []\n",
    "        \n",
    "        features_test = features_test.values.tolist()\n",
    "        \n",
    "        for row in features_test:\n",
    "            \n",
    "            test_features.append(row)\n",
    "\n",
    "        # Step 10. Combine all features into the original DataFrame.\n",
    "        \n",
    "        train_data['features'] = train_features\n",
    "        \n",
    "        test_data['features'] = test_features\n",
    "\n",
    "        # Step 11. Machine Learning\n",
    "        \n",
    "        train_features = np.vstack(train_data['features'].values)\n",
    "        \n",
    "        train_target = train_data['target'].values\n",
    "        \n",
    "        test_features = np.vstack(test_data['features'].values)\n",
    "        \n",
    "        test_target = test_data['target'].values\n",
    "\n",
    "        bc_accuracy = beef_classification_model(train_features, train_target, test_features, test_target)\n",
    "\n",
    "        # Step 12. Organize the classification results into a DataFrame.\n",
    "        \n",
    "        classification_accuracy = [bc_accuracy]\n",
    "        \n",
    "        all_classification_result.append(classification_accuracy)\n",
    "        \n",
    "        print('視窗大小 ', size,' 跑完嚕!')\n",
    "\n",
    "        print()\n",
    "\n",
    "    all_classification_result = np.array(all_classification_result)                                      # 轉成 np.array\n",
    "    \n",
    "    all_classification_result = pd.DataFrame(all_classification_result, columns = ['BaggingClassifier'])   # 轉成 dataframe\n",
    "    \n",
    "    all_classification_result.index = window_size\n",
    " \n",
    "    return all_classification_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "bb1be63a-621f-454f-a004-64c6a4c416af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coffee main experiment\n",
    "\n",
    "def coffee_experiment(embedding_dimension = 2):\n",
    "\n",
    "    all_classification_result = []   # Store all classification results.\n",
    "    \n",
    "    # Step 1. Call the coffee_data() method to obtain the beef dataset and convert it into Series format.\n",
    "    \n",
    "    train_data, test_data = coffee_data()\n",
    "\n",
    "    train_data_series = pd.Series(train_data['signal'].iloc[:])\n",
    "    \n",
    "    test_data_series = pd.Series(test_data['signal'].iloc[:])\n",
    "\n",
    "    window_size = [5, 10, 45, 50, 95, 100]        # (5, 10) = small size，(45, 50) = medium size，(95, 100) = large size\n",
    "\n",
    "    # Step 2. Apply a sliding window method to all time series in the dataset.\n",
    "    \n",
    "    for size in window_size:\n",
    "        \n",
    "        train_sliding_windows = train_data_series.map(lambda x: sliding_windows(x, size))\n",
    "        \n",
    "        test_sliding_windows = test_data_series.map(lambda x: sliding_windows(x, size))\n",
    "\n",
    "        # Step 3. Apply time-delay embedding method to each window and obtain a 2-dimensional point cloud.\n",
    "        \n",
    "        train_point_cloud = train_sliding_windows.map(lambda x: [takens_embedding(row, embedding_dimension) for row in x])\n",
    "        \n",
    "        test_point_cloud = test_sliding_windows.map(lambda x: [takens_embedding(row, embedding_dimension) for row in x])\n",
    "\n",
    "        # Step 4. Convert each point cloud into a persistence diagram.\n",
    "        \n",
    "        train_persistence_diagrams = train_point_cloud.map(lambda x: [persistent_homology(row) for row in x])\n",
    "        \n",
    "        test_persistence_diagrams = test_point_cloud.map(lambda x: [persistent_homology(row) for row in x])\n",
    "\n",
    "        # Step 5. Separate the H0 and H1 structures in the persistence diagram.\n",
    "        \n",
    "        train_H0 = train_persistence_diagrams.map(lambda x: [row[0] for row in x])\n",
    "        \n",
    "        train_H1 = train_persistence_diagrams.map(lambda x: [row[1] for row in x])\n",
    "\n",
    "        test_H0 = test_persistence_diagrams.map(lambda x: [row[0] for row in x])\n",
    "        \n",
    "        test_H1 = test_persistence_diagrams.map(lambda x: [row[1] for row in x])\n",
    "\n",
    "        # Step 6. Feature Extraction !\n",
    "\n",
    "        features_train_H0 = train_H0.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "        \n",
    "        features_train_H1 = train_H1.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "\n",
    "        features_test_H0 = test_H0.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "        \n",
    "        features_test_H1 = test_H1.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "\n",
    "        # Step 7. Use the itertools package to merge the features of each window into a 1-dimensional array.\n",
    "        \n",
    "        features_train_H0 = features_train_H0.map(lambda x: list(itertools.chain(*x)))\n",
    "        \n",
    "        features_train_H1 = features_train_H1.map(lambda x: list(itertools.chain(*x)))\n",
    "\n",
    "        features_test_H0 = features_test_H0.map(lambda x: list(itertools.chain(*x)))\n",
    "        \n",
    "        features_test_H1 = features_test_H1.map(lambda x: list(itertools.chain(*x)))\n",
    "\n",
    "        # Step 8. Convert all features of H0 and H1 into a DataFrame format, and merge them.\n",
    "        \n",
    "        features_train_H0 = pd_series_to_dataframe(features_train_H0)\n",
    "        \n",
    "        features_train_H1 = pd_series_to_dataframe(features_train_H1)\n",
    "\n",
    "        features_test_H0 = pd_series_to_dataframe(features_test_H0)\n",
    "        \n",
    "        features_test_H1 = pd_series_to_dataframe(features_test_H1)\n",
    "\n",
    "        features_train = pd.concat([features_train_H0, features_train_H1], axis=1)   #（主要解決兩個 Data Frame 合併後，列名重複問題！）\n",
    "        \n",
    "        features_train.columns = np.arange(features_train.shape[1])\n",
    "        \n",
    "        features_test = pd.concat([features_test_H0, features_test_H1], axis=1)\n",
    "        \n",
    "        features_test.columns = np.arange(features_test.shape[1])\n",
    "\n",
    "        # Step 9. Convert all features in each row into a list format.\n",
    "        \n",
    "        train_features = []\n",
    "        \n",
    "        features_train = features_train.values.tolist()\n",
    "        \n",
    "        for row in features_train:\n",
    "            \n",
    "            train_features.append(row)\n",
    "\n",
    "        test_features = []\n",
    "        \n",
    "        features_test = features_test.values.tolist()\n",
    "        \n",
    "        for row in features_test:\n",
    "            \n",
    "            test_features.append(row)\n",
    "\n",
    "        # Step 10. Combine all features into the original DataFrame.\n",
    "        \n",
    "        train_data['features'] = train_features\n",
    "        \n",
    "        test_data['features'] = test_features\n",
    "\n",
    "        # Step 11. Machine Learning\n",
    "        \n",
    "        train_features = np.vstack(train_data['features'].values)\n",
    "        \n",
    "        train_target = train_data['target'].values\n",
    "        \n",
    "        test_features = np.vstack(test_data['features'].values)\n",
    "        \n",
    "        test_target = test_data['target'].values\n",
    "\n",
    "        bc_accuracy = coffee_classification_model(train_features, train_target, test_features, test_target)\n",
    "\n",
    "        # Step 12. Organize the classification results into a DataFrame.\n",
    "        \n",
    "        classification_accuracy = [bc_accuracy]\n",
    "        \n",
    "        all_classification_result.append(classification_accuracy)\n",
    "        \n",
    "        print('視窗大小 ', size,' 跑完嚕!')\n",
    "\n",
    "        print()\n",
    "\n",
    "    all_classification_result = np.array(all_classification_result)                                      # 轉成 np.array\n",
    "    \n",
    "    all_classification_result = pd.DataFrame(all_classification_result, columns = ['BaggingClassifier'])   # 轉成 dataframe\n",
    "    \n",
    "    all_classification_result.index = window_size\n",
    " \n",
    "    return all_classification_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "3525c98b-1a37-4927-912d-406b6182fc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ham main experiment\n",
    "\n",
    "def ham_experiment(embedding_dimension = 2):\n",
    "\n",
    "    all_classification_result = []   # Store all classification results.\n",
    "    \n",
    "    # Step 1. Call the ham_data() method to obtain the beef dataset and convert it into Series format.\n",
    "    \n",
    "    train_data, test_data = ham_data()\n",
    "\n",
    "    train_data_series = pd.Series(train_data['signal'].iloc[:])\n",
    "    \n",
    "    test_data_series = pd.Series(test_data['signal'].iloc[:])\n",
    "\n",
    "    window_size = [5, 10, 45, 50, 95, 100]        # (5, 10) = small size，(45, 50) = medium size，(95, 100) = large size\n",
    "\n",
    "    # Step 2. Apply a sliding window method to all time series in the dataset.\n",
    "    \n",
    "    for size in window_size:\n",
    "        \n",
    "        train_sliding_windows = train_data_series.map(lambda x: sliding_windows(x, size))\n",
    "        \n",
    "        test_sliding_windows = test_data_series.map(lambda x: sliding_windows(x, size))\n",
    "\n",
    "        # Step 3. Apply time-delay embedding method to each window and obtain a 2-dimensional point cloud.\n",
    "        \n",
    "        train_point_cloud = train_sliding_windows.map(lambda x: [takens_embedding(row, embedding_dimension) for row in x])\n",
    "        \n",
    "        test_point_cloud = test_sliding_windows.map(lambda x: [takens_embedding(row, embedding_dimension) for row in x])\n",
    "\n",
    "        # Step 4. Convert each point cloud into a persistence diagram.\n",
    "        \n",
    "        train_persistence_diagrams = train_point_cloud.map(lambda x: [persistent_homology(row) for row in x])\n",
    "        \n",
    "        test_persistence_diagrams = test_point_cloud.map(lambda x: [persistent_homology(row) for row in x])\n",
    "\n",
    "        # Step 5. Separate the H0 and H1 structures in the persistence diagram.\n",
    "        \n",
    "        train_H0 = train_persistence_diagrams.map(lambda x: [row[0] for row in x])\n",
    "        \n",
    "        train_H1 = train_persistence_diagrams.map(lambda x: [row[1] for row in x])\n",
    "\n",
    "        test_H0 = test_persistence_diagrams.map(lambda x: [row[0] for row in x])\n",
    "        \n",
    "        test_H1 = test_persistence_diagrams.map(lambda x: [row[1] for row in x])\n",
    "\n",
    "        # Step 6. Feature Extraction !\n",
    "\n",
    "        features_train_H0 = train_H0.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "        \n",
    "        features_train_H1 = train_H1.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "\n",
    "        features_test_H0 = test_H0.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "        \n",
    "        features_test_H1 = test_H1.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "\n",
    "        # Step 7. Use the itertools package to merge the features of each window into a 1-dimensional array.\n",
    "        \n",
    "        features_train_H0 = features_train_H0.map(lambda x: list(itertools.chain(*x)))\n",
    "        \n",
    "        features_train_H1 = features_train_H1.map(lambda x: list(itertools.chain(*x)))\n",
    "\n",
    "        features_test_H0 = features_test_H0.map(lambda x: list(itertools.chain(*x)))\n",
    "        \n",
    "        features_test_H1 = features_test_H1.map(lambda x: list(itertools.chain(*x)))\n",
    "\n",
    "        # Step 8. Convert all features of H0 and H1 into a DataFrame format, and merge them.\n",
    "        \n",
    "        features_train_H0 = pd_series_to_dataframe(features_train_H0)\n",
    "        \n",
    "        features_train_H1 = pd_series_to_dataframe(features_train_H1)\n",
    "\n",
    "        features_test_H0 = pd_series_to_dataframe(features_test_H0)\n",
    "        \n",
    "        features_test_H1 = pd_series_to_dataframe(features_test_H1)\n",
    "\n",
    "        features_train = pd.concat([features_train_H0, features_train_H1], axis=1)   #（主要解決兩個 Data Frame 合併後，列名重複問題！）\n",
    "        \n",
    "        features_train.columns = np.arange(features_train.shape[1])\n",
    "        \n",
    "        features_test = pd.concat([features_test_H0, features_test_H1], axis=1)\n",
    "        \n",
    "        features_test.columns = np.arange(features_test.shape[1])\n",
    "\n",
    "        # Step 9. Convert all features in each row into a list format.\n",
    "        \n",
    "        train_features = []\n",
    "        \n",
    "        features_train = features_train.values.tolist()\n",
    "        \n",
    "        for row in features_train:\n",
    "            \n",
    "            train_features.append(row)\n",
    "\n",
    "        test_features = []\n",
    "        \n",
    "        features_test = features_test.values.tolist()\n",
    "        \n",
    "        for row in features_test:\n",
    "            \n",
    "            test_features.append(row)\n",
    "\n",
    "        # Step 10. Combine all features into the original DataFrame.\n",
    "        \n",
    "        train_data['features'] = train_features\n",
    "        \n",
    "        test_data['features'] = test_features\n",
    "\n",
    "        # Step 11. Machine Learning\n",
    "        \n",
    "        train_features = np.vstack(train_data['features'].values)\n",
    "        \n",
    "        train_target = train_data['target'].values\n",
    "        \n",
    "        test_features = np.vstack(test_data['features'].values)\n",
    "        \n",
    "        test_target = test_data['target'].values\n",
    "\n",
    "        bc_accuracy = ham_classification_model(train_features, train_target, test_features, test_target)\n",
    "\n",
    "        # Step 12. Organize the classification results into a DataFrame.\n",
    "        \n",
    "        classification_accuracy = [bc_accuracy]\n",
    "        \n",
    "        all_classification_result.append(classification_accuracy)\n",
    "        \n",
    "        print('視窗大小 ', size,' 跑完嚕!')\n",
    "\n",
    "        print()\n",
    "\n",
    "    all_classification_result = np.array(all_classification_result)                                      # 轉成 np.array\n",
    "    \n",
    "    all_classification_result = pd.DataFrame(all_classification_result, columns = ['BaggingClassifier'])   # 轉成 dataframe\n",
    "    \n",
    "    all_classification_result.index = window_size\n",
    " \n",
    "    return all_classification_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "fa4558ae-a803-49d9-99eb-7dded921b530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meat main experiment\n",
    "\n",
    "def meat_experiment(embedding_dimension = 2):\n",
    "\n",
    "    all_classification_result = []   # Store all classification results.\n",
    "    \n",
    "    # Step 1. Call the meat_data() method to obtain the beef dataset and convert it into Series format.\n",
    "    \n",
    "    train_data, test_data = meat_data()\n",
    "\n",
    "    train_data_series = pd.Series(train_data['signal'].iloc[:])\n",
    "    \n",
    "    test_data_series = pd.Series(test_data['signal'].iloc[:])\n",
    "\n",
    "    window_size = [5, 10, 45, 50, 95, 100]        # (5, 10) = small size，(45, 50) = medium size，(95, 100) = large size\n",
    "\n",
    "    # Step 2. Apply a sliding window method to all time series in the dataset.\n",
    "    \n",
    "    for size in window_size:\n",
    "        \n",
    "        train_sliding_windows = train_data_series.map(lambda x: sliding_windows(x, size))\n",
    "        \n",
    "        test_sliding_windows = test_data_series.map(lambda x: sliding_windows(x, size))\n",
    "\n",
    "        # Step 3. Apply time-delay embedding method to each window and obtain a 2-dimensional point cloud.\n",
    "        \n",
    "        train_point_cloud = train_sliding_windows.map(lambda x: [takens_embedding(row, embedding_dimension) for row in x])\n",
    "        \n",
    "        test_point_cloud = test_sliding_windows.map(lambda x: [takens_embedding(row, embedding_dimension) for row in x])\n",
    "\n",
    "        # Step 4. Convert each point cloud into a persistence diagram.\n",
    "        \n",
    "        train_persistence_diagrams = train_point_cloud.map(lambda x: [persistent_homology(row) for row in x])\n",
    "        \n",
    "        test_persistence_diagrams = test_point_cloud.map(lambda x: [persistent_homology(row) for row in x])\n",
    "\n",
    "        # Step 5. Separate the H0 and H1 structures in the persistence diagram.\n",
    "        \n",
    "        train_H0 = train_persistence_diagrams.map(lambda x: [row[0] for row in x])\n",
    "        \n",
    "        train_H1 = train_persistence_diagrams.map(lambda x: [row[1] for row in x])\n",
    "\n",
    "        test_H0 = test_persistence_diagrams.map(lambda x: [row[0] for row in x])\n",
    "        \n",
    "        test_H1 = test_persistence_diagrams.map(lambda x: [row[1] for row in x])\n",
    "\n",
    "        # Step 6. Feature Extraction !\n",
    "\n",
    "        features_train_H0 = train_H0.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "        \n",
    "        features_train_H1 = train_H1.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "\n",
    "        features_test_H0 = test_H0.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "        \n",
    "        features_test_H1 = test_H1.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "\n",
    "        # Step 7. Use the itertools package to merge the features of each window into a 1-dimensional array.\n",
    "        \n",
    "        features_train_H0 = features_train_H0.map(lambda x: list(itertools.chain(*x)))\n",
    "        \n",
    "        features_train_H1 = features_train_H1.map(lambda x: list(itertools.chain(*x)))\n",
    "\n",
    "        features_test_H0 = features_test_H0.map(lambda x: list(itertools.chain(*x)))\n",
    "        \n",
    "        features_test_H1 = features_test_H1.map(lambda x: list(itertools.chain(*x)))\n",
    "\n",
    "        # Step 8. Convert all features of H0 and H1 into a DataFrame format, and merge them.\n",
    "        \n",
    "        features_train_H0 = pd_series_to_dataframe(features_train_H0)\n",
    "        \n",
    "        features_train_H1 = pd_series_to_dataframe(features_train_H1)\n",
    "\n",
    "        features_test_H0 = pd_series_to_dataframe(features_test_H0)\n",
    "        \n",
    "        features_test_H1 = pd_series_to_dataframe(features_test_H1)\n",
    "\n",
    "        features_train = pd.concat([features_train_H0, features_train_H1], axis=1)   #（主要解決兩個 Data Frame 合併後，列名重複問題！）\n",
    "        \n",
    "        features_train.columns = np.arange(features_train.shape[1])\n",
    "        \n",
    "        features_test = pd.concat([features_test_H0, features_test_H1], axis=1)\n",
    "        \n",
    "        features_test.columns = np.arange(features_test.shape[1])\n",
    "\n",
    "        # Step 9. Convert all features in each row into a list format.\n",
    "        \n",
    "        train_features = []\n",
    "        \n",
    "        features_train = features_train.values.tolist()\n",
    "        \n",
    "        for row in features_train:\n",
    "            \n",
    "            train_features.append(row)\n",
    "\n",
    "        test_features = []\n",
    "        \n",
    "        features_test = features_test.values.tolist()\n",
    "        \n",
    "        for row in features_test:\n",
    "            \n",
    "            test_features.append(row)\n",
    "\n",
    "        # Step 10. Combine all features into the original DataFrame.\n",
    "        \n",
    "        train_data['features'] = train_features\n",
    "        \n",
    "        test_data['features'] = test_features\n",
    "\n",
    "        # Step 11. Machine Learning\n",
    "        \n",
    "        train_features = np.vstack(train_data['features'].values)\n",
    "        \n",
    "        train_target = train_data['target'].values\n",
    "        \n",
    "        test_features = np.vstack(test_data['features'].values)\n",
    "        \n",
    "        test_target = test_data['target'].values\n",
    "\n",
    "        bc_accuracy = meat_classification_model(train_features, train_target, test_features, test_target)\n",
    "\n",
    "        # Step 12. Organize the classification results into a DataFrame.\n",
    "        \n",
    "        classification_accuracy = [bc_accuracy]\n",
    "        \n",
    "        all_classification_result.append(classification_accuracy)\n",
    "        \n",
    "        print('視窗大小 ', size,' 跑完嚕!')\n",
    "\n",
    "        print()\n",
    "\n",
    "    all_classification_result = np.array(all_classification_result)                                      # 轉成 np.array\n",
    "    \n",
    "    all_classification_result = pd.DataFrame(all_classification_result, columns = ['BaggingClassifier'])   # 轉成 dataframe\n",
    "    \n",
    "    all_classification_result.index = window_size\n",
    " \n",
    "    return all_classification_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "18745075-f7a3-4f14-a54f-59284370e2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# olive oil main experiment\n",
    "\n",
    "def oliveoil_experiment(embedding_dimension = 2):\n",
    "\n",
    "    all_classification_result = []   # Store all classification results.\n",
    "    \n",
    "    # Step 1. Call the oliveoil_data() method to obtain the beef dataset and convert it into Series format.\n",
    "    \n",
    "    train_data, test_data = oliveoil_data()\n",
    "\n",
    "    train_data_series = pd.Series(train_data['signal'].iloc[:])\n",
    "    \n",
    "    test_data_series = pd.Series(test_data['signal'].iloc[:])\n",
    "\n",
    "    window_size = [5, 10, 45, 50, 95, 100]        # (5, 10) = small size，(45, 50) = medium size，(95, 100) = large size\n",
    "\n",
    "    # Step 2. Apply a sliding window method to all time series in the dataset.\n",
    "    \n",
    "    for size in window_size:\n",
    "        \n",
    "        train_sliding_windows = train_data_series.map(lambda x: sliding_windows(x, size))\n",
    "        \n",
    "        test_sliding_windows = test_data_series.map(lambda x: sliding_windows(x, size))\n",
    "\n",
    "        # Step 3. Apply time-delay embedding method to each window and obtain a 2-dimensional point cloud.\n",
    "        \n",
    "        train_point_cloud = train_sliding_windows.map(lambda x: [takens_embedding(row, embedding_dimension) for row in x])\n",
    "        \n",
    "        test_point_cloud = test_sliding_windows.map(lambda x: [takens_embedding(row, embedding_dimension) for row in x])\n",
    "\n",
    "        # Step 4. Convert each point cloud into a persistence diagram.\n",
    "        \n",
    "        train_persistence_diagrams = train_point_cloud.map(lambda x: [persistent_homology(row) for row in x])\n",
    "        \n",
    "        test_persistence_diagrams = test_point_cloud.map(lambda x: [persistent_homology(row) for row in x])\n",
    "\n",
    "        # Step 5. Separate the H0 and H1 structures in the persistence diagram.\n",
    "        \n",
    "        train_H0 = train_persistence_diagrams.map(lambda x: [row[0] for row in x])\n",
    "        \n",
    "        train_H1 = train_persistence_diagrams.map(lambda x: [row[1] for row in x])\n",
    "\n",
    "        test_H0 = test_persistence_diagrams.map(lambda x: [row[0] for row in x])\n",
    "        \n",
    "        test_H1 = test_persistence_diagrams.map(lambda x: [row[1] for row in x])\n",
    "\n",
    "        # Step 6. Feature Extraction !\n",
    "\n",
    "        features_train_H0 = train_H0.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "        \n",
    "        features_train_H1 = train_H1.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "\n",
    "        features_test_H0 = test_H0.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "        \n",
    "        features_test_H1 = test_H1.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "\n",
    "        # Step 7. Use the itertools package to merge the features of each window into a 1-dimensional array.\n",
    "        \n",
    "        features_train_H0 = features_train_H0.map(lambda x: list(itertools.chain(*x)))\n",
    "        \n",
    "        features_train_H1 = features_train_H1.map(lambda x: list(itertools.chain(*x)))\n",
    "\n",
    "        features_test_H0 = features_test_H0.map(lambda x: list(itertools.chain(*x)))\n",
    "        \n",
    "        features_test_H1 = features_test_H1.map(lambda x: list(itertools.chain(*x)))\n",
    "\n",
    "        # Step 8. Convert all features of H0 and H1 into a DataFrame format, and merge them.\n",
    "        \n",
    "        features_train_H0 = pd_series_to_dataframe(features_train_H0)\n",
    "        \n",
    "        features_train_H1 = pd_series_to_dataframe(features_train_H1)\n",
    "\n",
    "        features_test_H0 = pd_series_to_dataframe(features_test_H0)\n",
    "        \n",
    "        features_test_H1 = pd_series_to_dataframe(features_test_H1)\n",
    "\n",
    "        features_train = pd.concat([features_train_H0, features_train_H1], axis=1)   #（主要解決兩個 Data Frame 合併後，列名重複問題！）\n",
    "        \n",
    "        features_train.columns = np.arange(features_train.shape[1])\n",
    "        \n",
    "        features_test = pd.concat([features_test_H0, features_test_H1], axis=1)\n",
    "        \n",
    "        features_test.columns = np.arange(features_test.shape[1])\n",
    "\n",
    "        # Step 9. Convert all features in each row into a list format.\n",
    "        \n",
    "        train_features = []\n",
    "        \n",
    "        features_train = features_train.values.tolist()\n",
    "        \n",
    "        for row in features_train:\n",
    "            \n",
    "            train_features.append(row)\n",
    "\n",
    "        test_features = []\n",
    "        \n",
    "        features_test = features_test.values.tolist()\n",
    "        \n",
    "        for row in features_test:\n",
    "            \n",
    "            test_features.append(row)\n",
    "\n",
    "        # Step 10. Combine all features into the original DataFrame.\n",
    "        \n",
    "        train_data['features'] = train_features\n",
    "        \n",
    "        test_data['features'] = test_features\n",
    "\n",
    "        # Step 11. Machine Learning\n",
    "        \n",
    "        train_features = np.vstack(train_data['features'].values)\n",
    "        \n",
    "        train_target = train_data['target'].values\n",
    "        \n",
    "        test_features = np.vstack(test_data['features'].values)\n",
    "        \n",
    "        test_target = test_data['target'].values\n",
    "\n",
    "        bc_accuracy = oliveoil_classification_model(train_features, train_target, test_features, test_target)\n",
    "\n",
    "        # Step 12. Organize the classification results into a DataFrame.\n",
    "        \n",
    "        classification_accuracy = [bc_accuracy]\n",
    "        \n",
    "        all_classification_result.append(classification_accuracy)\n",
    "        \n",
    "        print('視窗大小 ', size,' 跑完嚕!')\n",
    "\n",
    "        print()\n",
    "\n",
    "    all_classification_result = np.array(all_classification_result)                                      # 轉成 np.array\n",
    "    \n",
    "    all_classification_result = pd.DataFrame(all_classification_result, columns = ['BaggingClassifier'])   # 轉成 dataframe\n",
    "    \n",
    "    all_classification_result.index = window_size\n",
    " \n",
    "    return all_classification_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "7a831997-4c38-4d2c-b3d8-b42fc0771c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# strawberry main experiment\n",
    "\n",
    "def strawberry_experiment(embedding_dimension = 2):\n",
    "\n",
    "    all_classification_result = []   # Store all classification results.\n",
    "    \n",
    "    # Step 1. Call the strawberry_data() method to obtain the beef dataset and convert it into Series format.\n",
    "    \n",
    "    train_data, test_data = strawberry_data()\n",
    "\n",
    "    train_data_series = pd.Series(train_data['signal'].iloc[:])\n",
    "    \n",
    "    test_data_series = pd.Series(test_data['signal'].iloc[:])\n",
    "\n",
    "    window_size = [5, 10, 45, 50, 95, 100]        # (5, 10) = small size，(45, 50) = medium size，(95, 100) = large size\n",
    "\n",
    "    # Step 2. Apply a sliding window method to all time series in the dataset.\n",
    "    \n",
    "    for size in window_size:\n",
    "        \n",
    "        train_sliding_windows = train_data_series.map(lambda x: sliding_windows(x, size))\n",
    "        \n",
    "        test_sliding_windows = test_data_series.map(lambda x: sliding_windows(x, size))\n",
    "\n",
    "        # Step 3. Apply time-delay embedding method to each window and obtain a 2-dimensional point cloud.\n",
    "        \n",
    "        train_point_cloud = train_sliding_windows.map(lambda x: [takens_embedding(row, embedding_dimension) for row in x])\n",
    "        \n",
    "        test_point_cloud = test_sliding_windows.map(lambda x: [takens_embedding(row, embedding_dimension) for row in x])\n",
    "\n",
    "        # Step 4. Convert each point cloud into a persistence diagram.\n",
    "        \n",
    "        train_persistence_diagrams = train_point_cloud.map(lambda x: [persistent_homology(row) for row in x])\n",
    "        \n",
    "        test_persistence_diagrams = test_point_cloud.map(lambda x: [persistent_homology(row) for row in x])\n",
    "\n",
    "        # Step 5. Separate the H0 and H1 structures in the persistence diagram.\n",
    "        \n",
    "        train_H0 = train_persistence_diagrams.map(lambda x: [row[0] for row in x])\n",
    "        \n",
    "        train_H1 = train_persistence_diagrams.map(lambda x: [row[1] for row in x])\n",
    "\n",
    "        test_H0 = test_persistence_diagrams.map(lambda x: [row[0] for row in x])\n",
    "        \n",
    "        test_H1 = test_persistence_diagrams.map(lambda x: [row[1] for row in x])\n",
    "\n",
    "        # Step 6. Feature Extraction !\n",
    "\n",
    "        features_train_H0 = train_H0.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "        \n",
    "        features_train_H1 = train_H1.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "\n",
    "        features_test_H0 = test_H0.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "        \n",
    "        features_test_H1 = test_H1.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "\n",
    "        # Step 7. Use the itertools package to merge the features of each window into a 1-dimensional array.\n",
    "        \n",
    "        features_train_H0 = features_train_H0.map(lambda x: list(itertools.chain(*x)))\n",
    "        \n",
    "        features_train_H1 = features_train_H1.map(lambda x: list(itertools.chain(*x)))\n",
    "\n",
    "        features_test_H0 = features_test_H0.map(lambda x: list(itertools.chain(*x)))\n",
    "        \n",
    "        features_test_H1 = features_test_H1.map(lambda x: list(itertools.chain(*x)))\n",
    "\n",
    "        # Step 8. Convert all features of H0 and H1 into a DataFrame format, and merge them.\n",
    "        \n",
    "        features_train_H0 = pd_series_to_dataframe(features_train_H0)\n",
    "        \n",
    "        features_train_H1 = pd_series_to_dataframe(features_train_H1)\n",
    "\n",
    "        features_test_H0 = pd_series_to_dataframe(features_test_H0)\n",
    "        \n",
    "        features_test_H1 = pd_series_to_dataframe(features_test_H1)\n",
    "\n",
    "        features_train = pd.concat([features_train_H0, features_train_H1], axis=1)   #（主要解決兩個 Data Frame 合併後，列名重複問題！）\n",
    "        \n",
    "        features_train.columns = np.arange(features_train.shape[1])\n",
    "        \n",
    "        features_test = pd.concat([features_test_H0, features_test_H1], axis=1)\n",
    "        \n",
    "        features_test.columns = np.arange(features_test.shape[1])\n",
    "\n",
    "        # Step 9. Convert all features in each row into a list format.\n",
    "        \n",
    "        train_features = []\n",
    "        \n",
    "        features_train = features_train.values.tolist()\n",
    "        \n",
    "        for row in features_train:\n",
    "            \n",
    "            train_features.append(row)\n",
    "\n",
    "        test_features = []\n",
    "        \n",
    "        features_test = features_test.values.tolist()\n",
    "        \n",
    "        for row in features_test:\n",
    "            \n",
    "            test_features.append(row)\n",
    "\n",
    "        # Step 10. Combine all features into the original DataFrame.\n",
    "        \n",
    "        train_data['features'] = train_features\n",
    "        \n",
    "        test_data['features'] = test_features\n",
    "\n",
    "        # Step 11. Machine Learning\n",
    "        \n",
    "        train_features = np.vstack(train_data['features'].values)\n",
    "        \n",
    "        train_target = train_data['target'].values\n",
    "        \n",
    "        test_features = np.vstack(test_data['features'].values)\n",
    "        \n",
    "        test_target = test_data['target'].values\n",
    "\n",
    "        bc_accuracy = strawberry_classification_model(train_features, train_target, test_features, test_target)\n",
    "\n",
    "        # Step 12. Organize the classification results into a DataFrame.\n",
    "        \n",
    "        classification_accuracy = [bc_accuracy]\n",
    "        \n",
    "        all_classification_result.append(classification_accuracy)\n",
    "        \n",
    "        print('視窗大小 ', size,' 跑完嚕!')\n",
    "\n",
    "        print()\n",
    "\n",
    "    all_classification_result = np.array(all_classification_result)                                      # 轉成 np.array\n",
    "    \n",
    "    all_classification_result = pd.DataFrame(all_classification_result, columns = ['BaggingClassifier'])   # 轉成 dataframe\n",
    "    \n",
    "    all_classification_result.index = window_size\n",
    " \n",
    "    return all_classification_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "c4fd0d46-0436-4b42-847a-62f432cf2c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wine main experiment\n",
    "\n",
    "def wine_experiment(embedding_dimension = 2):\n",
    "\n",
    "    all_classification_result = []   # Store all classification results.\n",
    "    \n",
    "    # Step 1. Call the wine_data() method to obtain the beef dataset and convert it into Series format.\n",
    "    \n",
    "    train_data, test_data = wine_data()\n",
    "\n",
    "    train_data_series = pd.Series(train_data['signal'].iloc[:])\n",
    "    \n",
    "    test_data_series = pd.Series(test_data['signal'].iloc[:])\n",
    "\n",
    "    window_size = [5, 10, 45, 50, 95, 100]        # (5, 10) = small size，(45, 50) = medium size，(95, 100) = large size\n",
    "\n",
    "    # Step 2. Apply a sliding window method to all time series in the dataset.\n",
    "    \n",
    "    for size in window_size:\n",
    "        \n",
    "        train_sliding_windows = train_data_series.map(lambda x: sliding_windows(x, size))\n",
    "        \n",
    "        test_sliding_windows = test_data_series.map(lambda x: sliding_windows(x, size))\n",
    "\n",
    "        # Step 3. Apply time-delay embedding method to each window and obtain a 2-dimensional point cloud.\n",
    "        \n",
    "        train_point_cloud = train_sliding_windows.map(lambda x: [takens_embedding(row, embedding_dimension) for row in x])\n",
    "        \n",
    "        test_point_cloud = test_sliding_windows.map(lambda x: [takens_embedding(row, embedding_dimension) for row in x])\n",
    "\n",
    "        # Step 4. Convert each point cloud into a persistence diagram.\n",
    "        \n",
    "        train_persistence_diagrams = train_point_cloud.map(lambda x: [persistent_homology(row) for row in x])\n",
    "        \n",
    "        test_persistence_diagrams = test_point_cloud.map(lambda x: [persistent_homology(row) for row in x])\n",
    "\n",
    "        # Step 5. Separate the H0 and H1 structures in the persistence diagram.\n",
    "        \n",
    "        train_H0 = train_persistence_diagrams.map(lambda x: [row[0] for row in x])\n",
    "        \n",
    "        train_H1 = train_persistence_diagrams.map(lambda x: [row[1] for row in x])\n",
    "\n",
    "        test_H0 = test_persistence_diagrams.map(lambda x: [row[0] for row in x])\n",
    "        \n",
    "        test_H1 = test_persistence_diagrams.map(lambda x: [row[1] for row in x])\n",
    "\n",
    "        # Step 6. Feature Extraction !\n",
    "\n",
    "        features_train_H0 = train_H0.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "        \n",
    "        features_train_H1 = train_H1.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "\n",
    "        features_test_H0 = test_H0.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "        \n",
    "        features_test_H1 = test_H1.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "\n",
    "        # Step 7. Use the itertools package to merge the features of each window into a 1-dimensional array.\n",
    "        \n",
    "        features_train_H0 = features_train_H0.map(lambda x: list(itertools.chain(*x)))\n",
    "        \n",
    "        features_train_H1 = features_train_H1.map(lambda x: list(itertools.chain(*x)))\n",
    "\n",
    "        features_test_H0 = features_test_H0.map(lambda x: list(itertools.chain(*x)))\n",
    "        \n",
    "        features_test_H1 = features_test_H1.map(lambda x: list(itertools.chain(*x)))\n",
    "\n",
    "        # Step 8. Convert all features of H0 and H1 into a DataFrame format, and merge them.\n",
    "        \n",
    "        features_train_H0 = pd_series_to_dataframe(features_train_H0)\n",
    "        \n",
    "        features_train_H1 = pd_series_to_dataframe(features_train_H1)\n",
    "\n",
    "        features_test_H0 = pd_series_to_dataframe(features_test_H0)\n",
    "        \n",
    "        features_test_H1 = pd_series_to_dataframe(features_test_H1)\n",
    "\n",
    "        features_train = pd.concat([features_train_H0, features_train_H1], axis=1)   #（主要解決兩個 Data Frame 合併後，列名重複問題！）\n",
    "        \n",
    "        features_train.columns = np.arange(features_train.shape[1])\n",
    "        \n",
    "        features_test = pd.concat([features_test_H0, features_test_H1], axis=1)\n",
    "        \n",
    "        features_test.columns = np.arange(features_test.shape[1])\n",
    "\n",
    "        # Step 9. Convert all features in each row into a list format.\n",
    "        \n",
    "        train_features = []\n",
    "        \n",
    "        features_train = features_train.values.tolist()\n",
    "        \n",
    "        for row in features_train:\n",
    "            \n",
    "            train_features.append(row)\n",
    "\n",
    "        test_features = []\n",
    "        \n",
    "        features_test = features_test.values.tolist()\n",
    "        \n",
    "        for row in features_test:\n",
    "            \n",
    "            test_features.append(row)\n",
    "\n",
    "        # Step 10. Combine all features into the original DataFrame.\n",
    "        \n",
    "        train_data['features'] = train_features\n",
    "        \n",
    "        test_data['features'] = test_features\n",
    "\n",
    "        # Step 11. Machine Learning\n",
    "        \n",
    "        train_features = np.vstack(train_data['features'].values)\n",
    "        \n",
    "        train_target = train_data['target'].values\n",
    "        \n",
    "        test_features = np.vstack(test_data['features'].values)\n",
    "        \n",
    "        test_target = test_data['target'].values\n",
    "\n",
    "        bc_accuracy = wine_classification_model(train_features, train_target, test_features, test_target)\n",
    "\n",
    "        # Step 12. Organize the classification results into a DataFrame.\n",
    "        \n",
    "        classification_accuracy = [bc_accuracy]\n",
    "        \n",
    "        all_classification_result.append(classification_accuracy)\n",
    "        \n",
    "        print('視窗大小 ', size,' 跑完嚕!')\n",
    "\n",
    "        print()\n",
    "\n",
    "    all_classification_result = np.array(all_classification_result)                                      # 轉成 np.array\n",
    "    \n",
    "    all_classification_result = pd.DataFrame(all_classification_result, columns = ['BaggingClassifier'])   # 轉成 dataframe\n",
    "    \n",
    "    all_classification_result.index = window_size\n",
    " \n",
    "    return all_classification_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154190cc-bf19-4af7-ae6e-a0231cf4aae4",
   "metadata": {},
   "source": [
    "## （7）Experimental results（研究結果）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "80fb4c2f-45e8-4d57-bccd-b071df172c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "最佳參數: {'bc__n_estimators': 50, 'bc__random_state': 14}\n",
      "最佳交叉驗證準確率: 0.5999999999999999\n",
      "bc Test accuracy: 0.9333333333333333\n",
      "視窗大小  5  跑完嚕!\n",
      "\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "最佳參數: {'bc__n_estimators': 50, 'bc__random_state': 14}\n",
      "最佳交叉驗證準確率: 0.6666666666666667\n",
      "bc Test accuracy: 0.8666666666666667\n",
      "視窗大小  10  跑完嚕!\n",
      "\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "最佳參數: {'bc__n_estimators': 50, 'bc__random_state': 14}\n",
      "最佳交叉驗證準確率: 0.5333333333333333\n",
      "bc Test accuracy: 0.7333333333333333\n",
      "視窗大小  45  跑完嚕!\n",
      "\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "最佳參數: {'bc__n_estimators': 50, 'bc__random_state': 14}\n",
      "最佳交叉驗證準確率: 0.5666666666666667\n",
      "bc Test accuracy: 0.7\n",
      "視窗大小  50  跑完嚕!\n",
      "\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "最佳參數: {'bc__n_estimators': 50, 'bc__random_state': 14}\n",
      "最佳交叉驗證準確率: 0.3666666666666666\n",
      "bc Test accuracy: 0.5333333333333333\n",
      "視窗大小  95  跑完嚕!\n",
      "\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "最佳參數: {'bc__n_estimators': 50, 'bc__random_state': 14}\n",
      "最佳交叉驗證準確率: 0.3\n",
      "bc Test accuracy: 0.5\n",
      "視窗大小  100  跑完嚕!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BaggingClassifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>93.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>86.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>73.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>70.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>53.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>50.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     BaggingClassifier\n",
       "5               93.333\n",
       "10              86.667\n",
       "45              73.333\n",
       "50              70.000\n",
       "95              53.333\n",
       "100             50.000"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beef_experiment = beef_experiment(embedding_dimension = 2)\n",
    "\n",
    "beef_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "1e701d9a-6dbb-450e-8403-b7420098f314",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Beef_Research_result.csv',  'w', encoding = 'utf = 8') as file:\n",
    "    \n",
    "     beef_experiment.to_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "27f10525-f230-484f-845c-c3c8b20a5dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "最佳參數: {'bc__max_samples': 1.0, 'bc__n_estimators': 100, 'bc__random_state': 14}\n",
      "最佳交叉驗證準確率: 0.9629629629629629\n",
      "bc Test accuracy: 0.8928571428571429\n",
      "視窗大小  5  跑完嚕!\n",
      "\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "最佳參數: {'bc__max_samples': 1.0, 'bc__n_estimators': 100, 'bc__random_state': 14}\n",
      "最佳交叉驗證準確率: 1.0\n",
      "bc Test accuracy: 1.0\n",
      "視窗大小  10  跑完嚕!\n",
      "\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "最佳參數: {'bc__max_samples': 1.0, 'bc__n_estimators': 100, 'bc__random_state': 14}\n",
      "最佳交叉驗證準確率: 0.9629629629629629\n",
      "bc Test accuracy: 0.9285714285714286\n",
      "視窗大小  45  跑完嚕!\n",
      "\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "最佳參數: {'bc__max_samples': 1.0, 'bc__n_estimators': 200, 'bc__random_state': 14}\n",
      "最佳交叉驗證準確率: 0.9666666666666667\n",
      "bc Test accuracy: 0.9285714285714286\n",
      "視窗大小  50  跑完嚕!\n",
      "\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "最佳參數: {'bc__max_samples': 0.8, 'bc__n_estimators': 200, 'bc__random_state': 14}\n",
      "最佳交叉驗證準確率: 0.8629629629629628\n",
      "bc Test accuracy: 1.0\n",
      "視窗大小  95  跑完嚕!\n",
      "\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "最佳參數: {'bc__max_samples': 1.0, 'bc__n_estimators': 100, 'bc__random_state': 14}\n",
      "最佳交叉驗證準確率: 0.825925925925926\n",
      "bc Test accuracy: 1.0\n",
      "視窗大小  100  跑完嚕!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BaggingClassifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>89.286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>100.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>92.857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>92.857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>100.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     BaggingClassifier\n",
       "5               89.286\n",
       "10             100.000\n",
       "45              92.857\n",
       "50              92.857\n",
       "95             100.000\n",
       "100            100.000"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coffee_experiment = coffee_experiment(embedding_dimension = 2)\n",
    "\n",
    "coffee_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "13de61b7-582e-47b9-be93-52e20518bf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Coffee_Research_result.csv',  'w', encoding = 'utf = 8') as file:\n",
    "    \n",
    "     coffee_experiment.to_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "43cc8b81-e14f-4ec1-badd-af0d7b78cfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "最佳參數: {'bc__max_features': 0.8, 'bc__max_samples': 0.8, 'bc__n_estimators': 10, 'bc__random_state': 45}\n",
      "最佳交叉驗證準確率: 0.5878378378378378\n",
      "bc Test accuracy: 0.6952380952380952\n",
      "視窗大小  5  跑完嚕!\n",
      "\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "最佳參數: {'bc__max_features': 0.8, 'bc__max_samples': 1.0, 'bc__n_estimators': 50, 'bc__random_state': 45}\n",
      "最佳交叉驗證準確率: 0.6331331331331331\n",
      "bc Test accuracy: 0.7333333333333333\n",
      "視窗大小  10  跑完嚕!\n",
      "\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "最佳參數: {'bc__max_features': 0.8, 'bc__max_samples': 1.0, 'bc__n_estimators': 20, 'bc__random_state': 45}\n",
      "最佳交叉驗證準確率: 0.6053553553553553\n",
      "bc Test accuracy: 0.8\n",
      "視窗大小  45  跑完嚕!\n",
      "\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "最佳參數: {'bc__max_features': 0.8, 'bc__max_samples': 0.8, 'bc__n_estimators': 10, 'bc__random_state': 45}\n",
      "最佳交叉驗證準確率: 0.6418918918918919\n",
      "bc Test accuracy: 0.7428571428571429\n",
      "視窗大小  50  跑完嚕!\n",
      "\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "最佳參數: {'bc__max_features': 0.8, 'bc__max_samples': 0.6, 'bc__n_estimators': 20, 'bc__random_state': 45}\n",
      "最佳交叉驗證準確率: 0.5227727727727728\n",
      "bc Test accuracy: 0.6476190476190476\n",
      "視窗大小  95  跑完嚕!\n",
      "\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "最佳參數: {'bc__max_features': 0.8, 'bc__max_samples': 0.6, 'bc__n_estimators': 20, 'bc__random_state': 45}\n",
      "最佳交叉驗證準確率: 0.504004004004004\n",
      "bc Test accuracy: 0.6666666666666666\n",
      "視窗大小  100  跑完嚕!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BaggingClassifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>69.524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>73.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>80.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>74.286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>64.762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>66.667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     BaggingClassifier\n",
       "5               69.524\n",
       "10              73.333\n",
       "45              80.000\n",
       "50              74.286\n",
       "95              64.762\n",
       "100             66.667"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_experiment = ham_experiment(embedding_dimension = 2)\n",
    "\n",
    "ham_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "c2ad1780-08c4-4fbb-8199-e39126a6a74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Ham_Research_result.csv',  'w', encoding = 'utf = 8') as file:\n",
    "    \n",
    "     ham_experiment.to_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "b8e20096-ddd6-48f8-9656-f51c09f36002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "最佳參數: {'bc__max_features': 1.0, 'bc__max_samples': 1.0, 'bc__n_estimators': 10, 'bc__random_state': 42}\n",
      "最佳交叉驗證準確率: 0.9833333333333334\n",
      "bc Test accuracy: 0.95\n",
      "視窗大小  5  跑完嚕!\n",
      "\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "最佳參數: {'bc__max_features': 1.0, 'bc__max_samples': 1.0, 'bc__n_estimators': 10, 'bc__random_state': 42}\n",
      "最佳交叉驗證準確率: 1.0\n",
      "bc Test accuracy: 0.95\n",
      "視窗大小  10  跑完嚕!\n",
      "\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "最佳參數: {'bc__max_features': 1.0, 'bc__max_samples': 0.8, 'bc__n_estimators': 10, 'bc__random_state': 42}\n",
      "最佳交叉驗證準確率: 1.0\n",
      "bc Test accuracy: 0.95\n",
      "視窗大小  45  跑完嚕!\n",
      "\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "最佳參數: {'bc__max_features': 1.0, 'bc__max_samples': 0.8, 'bc__n_estimators': 20, 'bc__random_state': 42}\n",
      "最佳交叉驗證準確率: 1.0\n",
      "bc Test accuracy: 0.95\n",
      "視窗大小  50  跑完嚕!\n",
      "\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "最佳參數: {'bc__max_features': 1.0, 'bc__max_samples': 1.0, 'bc__n_estimators': 10, 'bc__random_state': 42}\n",
      "最佳交叉驗證準確率: 0.9833333333333334\n",
      "bc Test accuracy: 0.9833333333333333\n",
      "視窗大小  95  跑完嚕!\n",
      "\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "最佳參數: {'bc__max_features': 1.0, 'bc__max_samples': 1.0, 'bc__n_estimators': 10, 'bc__random_state': 42}\n",
      "最佳交叉驗證準確率: 0.9833333333333334\n",
      "bc Test accuracy: 1.0\n",
      "視窗大小  100  跑完嚕!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BaggingClassifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>95.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>95.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>95.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>95.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>98.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>100.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     BaggingClassifier\n",
       "5               95.000\n",
       "10              95.000\n",
       "45              95.000\n",
       "50              95.000\n",
       "95              98.333\n",
       "100            100.000"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meat_experiment = meat_experiment(embedding_dimension = 2)\n",
    "\n",
    "meat_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "342cc80c-d1b5-4b7c-8eee-428649000e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Meat_Research_result.csv',  'w', encoding = 'utf = 8') as file:\n",
    "    \n",
    "     meat_experiment.to_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "1db5f3af-185a-43eb-a7fb-2961bd2a5f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳參數: {'bc__max_features': 1.0, 'bc__max_samples': 1.0, 'bc__n_estimators': 50, 'bc__random_state': 14}\n",
      "最佳交叉驗證準確率: 0.7666666666666666\n",
      "bc Test accuracy: 0.9\n",
      "視窗大小  5  跑完嚕!\n",
      "\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳參數: {'bc__max_features': 1.0, 'bc__max_samples': 0.6, 'bc__n_estimators': 10, 'bc__random_state': 14}\n",
      "最佳交叉驗證準確率: 0.8333333333333334\n",
      "bc Test accuracy: 0.9666666666666667\n",
      "視窗大小  10  跑完嚕!\n",
      "\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "最佳參數: {'bc__max_features': 1.0, 'bc__max_samples': 1.0, 'bc__n_estimators': 100, 'bc__random_state': 14}\n",
      "最佳交叉驗證準確率: 0.9\n",
      "bc Test accuracy: 0.9333333333333333\n",
      "視窗大小  45  跑完嚕!\n",
      "\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳參數: {'bc__max_features': 1.0, 'bc__max_samples': 0.6, 'bc__n_estimators': 30, 'bc__random_state': 14}\n",
      "最佳交叉驗證準確率: 0.9\n",
      "bc Test accuracy: 0.9666666666666667\n",
      "視窗大小  50  跑完嚕!\n",
      "\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "最佳參數: {'bc__max_features': 0.8, 'bc__max_samples': 1.0, 'bc__n_estimators': 10, 'bc__random_state': 14}\n",
      "最佳交叉驗證準確率: 0.9\n",
      "bc Test accuracy: 0.9666666666666667\n",
      "視窗大小  95  跑完嚕!\n",
      "\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "最佳參數: {'bc__max_features': 1.0, 'bc__max_samples': 1.0, 'bc__n_estimators': 10, 'bc__random_state': 14}\n",
      "最佳交叉驗證準確率: 0.8666666666666667\n",
      "bc Test accuracy: 0.9666666666666667\n",
      "視窗大小  100  跑完嚕!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BaggingClassifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>90.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>96.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>93.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>96.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>96.667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     BaggingClassifier\n",
       "5               90.000\n",
       "10              96.667\n",
       "45              93.333\n",
       "50              96.667\n",
       "95              96.667\n",
       "100             96.667"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oliveoil_experiment = oliveoil_experiment(embedding_dimension = 2)\n",
    "\n",
    "oliveoil_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "4c2e13eb-bea4-45d2-bc36-aada48182ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Oliveoil_Research_result.csv',  'w', encoding = 'utf = 8') as file:\n",
    "    \n",
    "     oliveoil_experiment.to_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "ae9bd8ef-a622-41da-b56e-e69090838d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳參數: {'bc__max_features': 1.0, 'bc__max_samples': 1.0, 'bc__n_estimators': 20, 'bc__random_state': 14}\n",
      "最佳交叉驗證準確率: 0.9509963334927467\n",
      "bc Test accuracy: 0.9783783783783784\n",
      "視窗大小  5  跑完嚕!\n",
      "\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳參數: {'bc__max_features': 1.0, 'bc__max_samples': 1.0, 'bc__n_estimators': 20, 'bc__random_state': 14}\n",
      "最佳交叉驗證準確率: 0.9412083532600032\n",
      "bc Test accuracy: 0.9567567567567568\n",
      "視窗大小  10  跑完嚕!\n",
      "\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳參數: {'bc__max_features': 1.0, 'bc__max_samples': 0.8, 'bc__n_estimators': 30, 'bc__random_state': 14}\n",
      "最佳交叉驗證準確率: 0.9248923959827833\n",
      "bc Test accuracy: 0.9432432432432433\n",
      "視窗大小  45  跑完嚕!\n",
      "\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "最佳參數: {'bc__max_features': 1.0, 'bc__max_samples': 0.6, 'bc__n_estimators': 20, 'bc__random_state': 14}\n",
      "最佳交叉驗證準確率: 0.9200063765343535\n",
      "bc Test accuracy: 0.927027027027027\n",
      "視窗大小  50  跑完嚕!\n",
      "\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "最佳參數: {'bc__max_features': 1.0, 'bc__max_samples': 1.0, 'bc__n_estimators': 50, 'bc__random_state': 14}\n",
      "最佳交叉驗證準確率: 0.8955364259524948\n",
      "bc Test accuracy: 0.9108108108108108\n",
      "視窗大小  95  跑完嚕!\n",
      "\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "最佳參數: {'bc__max_features': 1.0, 'bc__max_samples': 0.8, 'bc__n_estimators': 50, 'bc__random_state': 14}\n",
      "最佳交叉驗證準確率: 0.8971544715447154\n",
      "bc Test accuracy: 0.9081081081081082\n",
      "視窗大小  100  跑完嚕!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BaggingClassifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>97.838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>95.676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>94.324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>92.703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>91.081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>90.811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     BaggingClassifier\n",
       "5               97.838\n",
       "10              95.676\n",
       "45              94.324\n",
       "50              92.703\n",
       "95              91.081\n",
       "100             90.811"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strawberry_experiment = strawberry_experiment(embedding_dimension = 2)\n",
    "\n",
    "strawberry_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "0681b75b-5910-4b03-ad2e-07874677fa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Strawberry_Research_result.csv',  'w', encoding = 'utf = 8') as file:\n",
    "    \n",
    "     strawberry_experiment.to_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "34761216-b4d9-41d1-aeb2-7c26291783ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "最佳參數: {'bc__max_samples': 0.6, 'bc__n_estimators': 30, 'bc__random_state': 3}\n",
      "最佳交叉驗證準確率: 0.7017543859649122\n",
      "bc Test accuracy: 0.8333333333333334\n",
      "視窗大小  5  跑完嚕!\n",
      "\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "最佳參數: {'bc__max_samples': 1.0, 'bc__n_estimators': 30, 'bc__random_state': 3}\n",
      "最佳交叉驗證準確率: 0.7894736842105264\n",
      "bc Test accuracy: 0.9259259259259259\n",
      "視窗大小  10  跑完嚕!\n",
      "\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "最佳參數: {'bc__max_samples': 0.8, 'bc__n_estimators': 10, 'bc__random_state': 3}\n",
      "最佳交叉驗證準確率: 0.6140350877192983\n",
      "bc Test accuracy: 0.6481481481481481\n",
      "視窗大小  45  跑完嚕!\n",
      "\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "最佳參數: {'bc__max_samples': 1.0, 'bc__n_estimators': 100, 'bc__random_state': 3}\n",
      "最佳交叉驗證準確率: 0.6842105263157895\n",
      "bc Test accuracy: 0.7037037037037037\n",
      "視窗大小  50  跑完嚕!\n",
      "\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "最佳參數: {'bc__max_samples': 1.0, 'bc__n_estimators': 30, 'bc__random_state': 3}\n",
      "最佳交叉驗證準確率: 0.6842105263157894\n",
      "bc Test accuracy: 0.8333333333333334\n",
      "視窗大小  95  跑完嚕!\n",
      "\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "最佳參數: {'bc__max_samples': 1.0, 'bc__n_estimators': 30, 'bc__random_state': 3}\n",
      "最佳交叉驗證準確率: 0.6666666666666666\n",
      "bc Test accuracy: 0.7962962962962963\n",
      "視窗大小  100  跑完嚕!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BaggingClassifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>83.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>92.593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>64.815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>70.370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>83.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>79.630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     BaggingClassifier\n",
       "5               83.333\n",
       "10              92.593\n",
       "45              64.815\n",
       "50              70.370\n",
       "95              83.333\n",
       "100             79.630"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_experiment = wine_experiment(embedding_dimension = 2)\n",
    "\n",
    "wine_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "1a8c4ffa-6500-481f-b05e-0dbeb1cb577c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Wine_Research_result.csv',  'w', encoding = 'utf = 8') as file:\n",
    "    \n",
    "     wine_experiment.to_csv(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
