{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe9b3020-ba86-4cfb-960c-36c853c3025f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from gtda.time_series import SingleTakensEmbedding     # Implement the sliding window and the time delay embedding method.\n",
    "\n",
    "from ripser import ripser                              # The Ripser package is used to compute Persistent Homology.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import itertools                                       # This package is primarily used to handle iterable objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6521540-d315-4ad1-a2b2-8ba5771592ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## （1）Sliding Window Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4046ec11-713f-47ae-b5c7-14f3ed1e436e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sliding shift = 1 \n",
    "\n",
    "# signal is a 1-dimensional array of time series data.\n",
    "\n",
    "def sliding_windows(signal, window_size):\n",
    "    \n",
    "    window = SingleTakensEmbedding(parameters_type = \"fixed\", dimension = window_size)\n",
    "    \n",
    "    sliding_windows = window.fit_transform(signal)\n",
    "    \n",
    "    return sliding_windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c899f542-d82f-4258-997d-cefea21bcb88",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## （2）TDA and Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fa6210-9672-40c5-87e5-331cece65b0d",
   "metadata": {},
   "source": [
    "### <font color = SteelBlue >Takens Embedding and Persistent Homology</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "657ebebd-0a80-4af8-b6a0-8206a78376d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#                                                                                                                                   #\n",
    "#  The takens_embedding method transforms a time series signal into a point cloud.                                                  #\n",
    "#                                                                                                                                   #\n",
    "#  The persistent_homology method is used to compute Persistent Homology and transforms a point cloudd into a persistence diagram.  #\n",
    "#                                                                                                                                   #\n",
    "#####################################################################################################################################\n",
    "\n",
    "def takens_embedding(signal, embedding_dimension):\n",
    "    \n",
    "    embedder = SingleTakensEmbedding(parameters_type = \"fixed\", dimension = embedding_dimension)\n",
    "    \n",
    "    point_cloud = embedder.fit_transform(signal)\n",
    "    \n",
    "    return point_cloud\n",
    "\n",
    "def persistent_homology(point_cloud):\n",
    "    \n",
    "    persistence_diagram = ripser(point_cloud)['dgms']\n",
    "    \n",
    "    return persistence_diagram "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f9b6e8-ee21-4c1d-b1b3-09be8bcc34ff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### <font color = SteelBlue >Feature Extraction 1：get_diagram_lives</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d68fed22-1259-4163-ac5d-17c6844e6e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the life(death - birth) of all points in the persistence diagram. \n",
    "\n",
    "def get_diagram_lives(diagram):\n",
    "    \n",
    "    return diagram[:,1] - diagram[:,0] if len(diagram) > 0 else np.array([0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc92872-b8ee-4eca-b648-e1a80184a81c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### <font color = SteelBlue >Feature Extraction 2：get_diagram_entropy</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c24ea70-4ec1-4f0b-874b-f0b2c19a86f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the persistence entropy.\n",
    "\n",
    "def get_diagram_entropy(lives):\n",
    "    \n",
    "    if max(abs(lives)) == 0:\n",
    "        \n",
    "        return 0.\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        normalized_lives = lives / sum(lives)\n",
    "        \n",
    "        return sum(-normalized_lives * np.log(normalized_lives))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e05d98-a380-4c83-8ed1-3a69902a2196",
   "metadata": {},
   "source": [
    "### <font color = SteelBlue >Feature Extraction 3：get_diagram_features</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41025d35-2d94-42e8-96b2-c86d233202f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The get_diagram_features method organizes all three features into a single array.\n",
    "\n",
    "def get_diagram_features(diagram):\n",
    "    \n",
    "    diagram = diagram[~np.any(np.isinf(diagram), axis = 1)]   # Remove points with inf values from the persistence diagram (for H0).\n",
    "    \n",
    "    lives = get_diagram_lives(diagram)\n",
    "    \n",
    "    bottleneck_distance = max(lives) / np.sqrt(2)        # feature 1（ bottleneck distance ）\n",
    "    \n",
    "    wasserstein_distance = sum(lives) / np.sqrt(2)       # feature 2（ wasserstein distance ）\n",
    "    \n",
    "    persistence_entropy = get_diagram_entropy(lives)     # feature 3（ persistence entropy ）\n",
    "    \n",
    "    result = [bottleneck_distance, wasserstein_distance, persistence_entropy] \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e8c749-a09b-44c2-9c37-1c402cddef1b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## （3）UCR Time Series Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b1f0ed8-0640-43c0-b05e-4b2d697558aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method splits the dataset into training and testing sets and converts them into DataFrames.\n",
    "\n",
    "def beef_data():\n",
    "    \n",
    "    # Read a txt file and convert it into a DataFrame.\n",
    "    \n",
    "    data_train = pd.read_csv('../UCR_datasets/Beef_TRAIN.txt', sep = '\\s+', header = None)\n",
    "    \n",
    "    data_test = pd.read_csv('../UCR_datasets/Beef_TEST.txt', sep = '\\s+', header = None)\n",
    "    \n",
    "    # Separate the targets from the time series signal.\n",
    "    \n",
    "    train_targets = data_train.iloc[:, 0].astype(int)  \n",
    "    \n",
    "    train_signals = data_train.iloc[:, 1:].astype(float)  \n",
    "\n",
    "    test_targets = data_test.iloc[:, 0].astype(int)  \n",
    "    \n",
    "    test_signals = data_test.iloc[:, 1:].astype(float)  \n",
    "\n",
    "    # Convert the time series data in each row into a list format.\n",
    "    \n",
    "    train_signalss = []\n",
    "    \n",
    "    test_signalss = []\n",
    "    \n",
    "    train_signals_list = train_signals.values.tolist()\n",
    "    \n",
    "    for row in train_signals_list:\n",
    "        \n",
    "        train_signalss.append(row)\n",
    "    \n",
    "    test_signals_list = test_signals.values.tolist()\n",
    "    \n",
    "    for row in test_signals_list:\n",
    "        \n",
    "        test_signalss.append(row)\n",
    "\n",
    "    # Create a new DataFrame where the signal is in list format.\n",
    "    \n",
    "    df_train = pd.DataFrame({\n",
    "        \n",
    "        'signal': train_signalss,\n",
    "        \n",
    "        'target': train_targets\n",
    "        \n",
    "    })\n",
    "\n",
    "    df_test = pd.DataFrame({\n",
    "        \n",
    "        'signal': test_signalss,\n",
    "        \n",
    "        'target': test_targets\n",
    "        \n",
    "    })\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d9c0bdf-948c-4d59-b37f-60c4f27a88d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method splits the dataset into training and testing sets and converts them into DataFrames.\n",
    "\n",
    "def coffee_data():\n",
    "    \n",
    "    # Read a txt file and convert it into a DataFrame.\n",
    "    \n",
    "    data_train = pd.read_csv('../UCR_datasets/Coffee_TRAIN.txt', sep='\\s+', header=None)\n",
    "    \n",
    "    data_test = pd.read_csv('../UCR_datasets/Coffee_TEST.txt', sep='\\s+', header=None)\n",
    "    \n",
    "    # Separate the targets from the time series signal.\n",
    "    \n",
    "    train_targets = data_train.iloc[:, 0].astype(int)  \n",
    "    \n",
    "    train_signals = data_train.iloc[:, 1:].astype(float)  \n",
    "\n",
    "    test_targets = data_test.iloc[:, 0].astype(int)  \n",
    "    \n",
    "    test_signals = data_test.iloc[:, 1:].astype(float)  \n",
    "\n",
    "    # Convert the time series data in each row into a list format.\n",
    "    \n",
    "    train_signalss = []\n",
    "    \n",
    "    test_signalss = []\n",
    "    \n",
    "    train_signals_list = train_signals.values.tolist()\n",
    "    \n",
    "    for row in train_signals_list:\n",
    "        \n",
    "        train_signalss.append(row)\n",
    "    \n",
    "    test_signals_list = test_signals.values.tolist()\n",
    "    \n",
    "    for row in test_signals_list:\n",
    "        \n",
    "        test_signalss.append(row)\n",
    "\n",
    "    # Create a new DataFrame where the signal is in list format.\n",
    "    \n",
    "    df_train = pd.DataFrame({\n",
    "        \n",
    "        'signal': train_signalss,\n",
    "        \n",
    "        'target': train_targets\n",
    "        \n",
    "    })\n",
    "\n",
    "    df_test = pd.DataFrame({\n",
    "        \n",
    "        'signal': test_signalss,\n",
    "        \n",
    "        'target': test_targets\n",
    "        \n",
    "    })\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fcbb969-020f-4a7f-9ad5-ed87439da4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method splits the dataset into training and testing sets and converts them into DataFrames.\n",
    "\n",
    "def ham_data():\n",
    "    \n",
    "    # Read a txt file and convert it into a DataFrame.\n",
    "    \n",
    "    data_train = pd.read_csv('../UCR_datasets/Ham_TRAIN.txt', sep='\\s+', header=None)\n",
    "    \n",
    "    data_test = pd.read_csv('../UCR_datasets/Ham_TEST.txt', sep='\\s+', header=None)\n",
    "    \n",
    "    # Separate the targets from the time series signal.\n",
    "    \n",
    "    train_targets = data_train.iloc[:, 0].astype(int)  \n",
    "    \n",
    "    train_signals = data_train.iloc[:, 1:].astype(float)  \n",
    "\n",
    "    test_targets = data_test.iloc[:, 0].astype(int)  \n",
    "    \n",
    "    test_signals = data_test.iloc[:, 1:].astype(float)  \n",
    "\n",
    "    # Convert the time series data in each row into a list format.\n",
    "    \n",
    "    train_signalss = []\n",
    "    \n",
    "    test_signalss = []\n",
    "    \n",
    "    train_signals_list = train_signals.values.tolist()\n",
    "    \n",
    "    for row in train_signals_list:\n",
    "        \n",
    "        train_signalss.append(row)\n",
    "    \n",
    "    test_signals_list = test_signals.values.tolist()\n",
    "    \n",
    "    for row in test_signals_list:\n",
    "        \n",
    "        test_signalss.append(row)\n",
    "\n",
    "    # Create a new DataFrame where the signal is in list format.\n",
    "    \n",
    "    df_train = pd.DataFrame({\n",
    "        \n",
    "        'signal': train_signalss,\n",
    "        \n",
    "        'target': train_targets\n",
    "        \n",
    "    })\n",
    "\n",
    "    df_test = pd.DataFrame({\n",
    "        \n",
    "        'signal': test_signalss,\n",
    "        \n",
    "        'target': test_targets\n",
    "        \n",
    "    })\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9561c4c7-d93b-433e-a0ce-6d5a49e024a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method splits the dataset into training and testing sets and converts them into DataFrames.\n",
    "\n",
    "def meat_data():\n",
    "    \n",
    "    # Read a txt file and convert it into a DataFrame.\n",
    "    \n",
    "    data_train = pd.read_csv('../UCR_datasets/Meat_TRAIN.txt', sep='\\s+', header=None)\n",
    "    \n",
    "    data_test = pd.read_csv('../UCR_datasets/Meat_TEST.txt', sep='\\s+', header=None)\n",
    "    \n",
    "    # Separate the targets from the time series signal.\n",
    "    \n",
    "    train_targets = data_train.iloc[:, 0].astype(int)  \n",
    "    \n",
    "    train_signals = data_train.iloc[:, 1:].astype(float)  \n",
    "\n",
    "    test_targets = data_test.iloc[:, 0].astype(int)  \n",
    "    \n",
    "    test_signals = data_test.iloc[:, 1:].astype(float)  \n",
    "\n",
    "    # Convert the time series data in each row into a list format.\n",
    "    \n",
    "    train_signalss = []\n",
    "    \n",
    "    test_signalss = []\n",
    "    \n",
    "    train_signals_list = train_signals.values.tolist()\n",
    "    \n",
    "    for row in train_signals_list:\n",
    "        \n",
    "        train_signalss.append(row)\n",
    "    \n",
    "    test_signals_list = test_signals.values.tolist()\n",
    "    \n",
    "    for row in test_signals_list:\n",
    "        \n",
    "        test_signalss.append(row)\n",
    "\n",
    "    # Create a new DataFrame where the signal is in list format.\n",
    "    \n",
    "    df_train = pd.DataFrame({\n",
    "        \n",
    "        'signal': train_signalss,\n",
    "        \n",
    "        'target': train_targets\n",
    "        \n",
    "    })\n",
    "\n",
    "    df_test = pd.DataFrame({\n",
    "        \n",
    "        'signal': test_signalss,\n",
    "        \n",
    "        'target': test_targets\n",
    "        \n",
    "    })\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b3c33a1-55ee-4b84-b461-48a6e6d2f0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method splits the dataset into training and testing sets and converts them into DataFrames.\n",
    "\n",
    "def oliveoil_data():\n",
    "    \n",
    "    # Read a txt file and convert it into a DataFrame.\n",
    "    \n",
    "    data_train = pd.read_csv('../UCR_datasets/OliveOil_TRAIN.txt', sep='\\s+', header=None)\n",
    "    \n",
    "    data_test = pd.read_csv('../UCR_datasets/OliveOil_TEST.txt', sep='\\s+', header=None)\n",
    "    \n",
    "    # Separate the targets from the time series signal.\n",
    "    \n",
    "    train_targets = data_train.iloc[:, 0].astype(int)  \n",
    "    \n",
    "    train_signals = data_train.iloc[:, 1:].astype(float)  \n",
    "\n",
    "    test_targets = data_test.iloc[:, 0].astype(int)  \n",
    "    \n",
    "    test_signals = data_test.iloc[:, 1:].astype(float)  \n",
    "\n",
    "    # Convert the time series data in each row into a list format.\n",
    "    \n",
    "    train_signalss = []\n",
    "    \n",
    "    test_signalss = []\n",
    "    \n",
    "    train_signals_list = train_signals.values.tolist()\n",
    "    \n",
    "    for row in train_signals_list:\n",
    "        \n",
    "        train_signalss.append(row)\n",
    "    \n",
    "    test_signals_list = test_signals.values.tolist()\n",
    "    \n",
    "    for row in test_signals_list:\n",
    "        \n",
    "        test_signalss.append(row)\n",
    "\n",
    "    # Create a new DataFrame where the signal is in list format.\n",
    "    \n",
    "    df_train = pd.DataFrame({\n",
    "        \n",
    "        'signal': train_signalss,\n",
    "        \n",
    "        'target': train_targets\n",
    "        \n",
    "    })\n",
    "\n",
    "    df_test = pd.DataFrame({\n",
    "        \n",
    "        'signal': test_signalss,\n",
    "        \n",
    "        'target': test_targets\n",
    "        \n",
    "    })\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85f4a5e2-b72f-40d9-9917-7dc72a980598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method splits the dataset into training and testing sets and converts them into DataFrames.\n",
    "\n",
    "def strawberry_data():\n",
    "    \n",
    "    # Read a txt file and convert it into a DataFrame.\n",
    "    \n",
    "    data_train = pd.read_csv('../UCR_datasets/Strawberry_TRAIN.txt', sep='\\s+', header=None)\n",
    "    \n",
    "    data_test = pd.read_csv('../UCR_datasets/Strawberry_TEST.txt', sep='\\s+', header=None)\n",
    "    \n",
    "    # Separate the targets from the time series signal.\n",
    "    \n",
    "    train_targets = data_train.iloc[:, 0].astype(int)  \n",
    "    \n",
    "    train_signals = data_train.iloc[:, 1:].astype(float)  \n",
    "\n",
    "    test_targets = data_test.iloc[:, 0].astype(int)  \n",
    "    \n",
    "    test_signals = data_test.iloc[:, 1:].astype(float)  \n",
    "\n",
    "    # Convert the time series data in each row into a list format.\n",
    "    \n",
    "    train_signalss = []\n",
    "    \n",
    "    test_signalss = []\n",
    "    \n",
    "    train_signals_list = train_signals.values.tolist()\n",
    "    \n",
    "    for row in train_signals_list:\n",
    "        \n",
    "        train_signalss.append(row)\n",
    "    \n",
    "    test_signals_list = test_signals.values.tolist()\n",
    "    \n",
    "    for row in test_signals_list:\n",
    "        \n",
    "        test_signalss.append(row)\n",
    "\n",
    "    # Create a new DataFrame where the signal is in list format.\n",
    "    \n",
    "    df_train = pd.DataFrame({\n",
    "        \n",
    "        'signal': train_signalss,\n",
    "        \n",
    "        'target': train_targets\n",
    "        \n",
    "    })\n",
    "\n",
    "    df_test = pd.DataFrame({\n",
    "        \n",
    "        'signal': test_signalss,\n",
    "        \n",
    "        'target': test_targets\n",
    "        \n",
    "    })\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3abfd7ba-dfee-4979-a409-34d4388da64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method splits the dataset into training and testing sets and converts them into DataFrames.\n",
    "\n",
    "def wine_data():\n",
    "    \n",
    "    # Read a txt file and convert it into a DataFrame.\n",
    "    \n",
    "    data_train = pd.read_csv('../UCR_datasets/Wine_TRAIN.txt', sep='\\s+', header=None)\n",
    "    \n",
    "    data_test = pd.read_csv('../UCR_datasets/Wine_TEST.txt', sep='\\s+', header=None)\n",
    "    \n",
    "    # Separate the targets from the time series signal.\n",
    "    \n",
    "    train_targets = data_train.iloc[:, 0].astype(int)  \n",
    "    \n",
    "    train_signals = data_train.iloc[:, 1:].astype(float)  \n",
    "\n",
    "    test_targets = data_test.iloc[:, 0].astype(int)  \n",
    "    \n",
    "    test_signals = data_test.iloc[:, 1:].astype(float)  \n",
    "\n",
    "    # Convert the time series data in each row into a list format.\n",
    "    \n",
    "    train_signalss = []\n",
    "    \n",
    "    test_signalss = []\n",
    "    \n",
    "    train_signals_list = train_signals.values.tolist()\n",
    "    \n",
    "    for row in train_signals_list:\n",
    "        \n",
    "        train_signalss.append(row)\n",
    "    \n",
    "    test_signals_list = test_signals.values.tolist()\n",
    "    \n",
    "    for row in test_signals_list:\n",
    "        \n",
    "        test_signalss.append(row)\n",
    "\n",
    "    # Create a new DataFrame where the signal is in list format.\n",
    "    \n",
    "    df_train = pd.DataFrame({\n",
    "        \n",
    "        'signal': train_signalss,\n",
    "        \n",
    "        'target': train_targets\n",
    "        \n",
    "    })\n",
    "\n",
    "    df_test = pd.DataFrame({\n",
    "        \n",
    "        'signal': test_signalss,\n",
    "        \n",
    "        'target': test_targets\n",
    "        \n",
    "    })\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d56a61-27b9-4e99-a02b-0b0a9436ddd2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## （4）Machine Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66c86ec6-ff31-4eb5-9ede-2a12e7465fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "332239a9-c278-4a99-b347-4841b8c4ee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Beef\n",
    "\n",
    "def beef_classification_model(train_features, train_target, test_features, test_target):\n",
    "    \n",
    "    X_train = train_features\n",
    "    \n",
    "    y_train = train_target\n",
    "    \n",
    "    X_test = test_features\n",
    "    \n",
    "    y_test = test_target\n",
    "\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), ('bc', BaggingClassifier(estimator = LogisticRegression(max_iter = 3000, C = 0.01)))])\n",
    "    \n",
    "    param_grid = {\n",
    "        \n",
    "    'bc__n_estimators': [50],\n",
    "        \n",
    "    'bc__random_state': [14]\n",
    "        \n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(pipe, param_grid, cv = 5, scoring = 'accuracy', verbose = 1, n_jobs = -1)\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    print(f\"Best cross-validation accuracy: {grid_search.best_score_}\")\n",
    "    \n",
    "    y_pred = grid_search.predict(X_test)\n",
    "\n",
    "    bc_accuracy = round(100 * (accuracy_score(y_test, y_pred)), 3)\n",
    "\n",
    "    bc_macro_f1 = round(100 * (f1_score(y_test, y_pred, average=\"macro\")), 3)\n",
    "    \n",
    "    print('bc Test accuracy:', accuracy_score(y_test, y_pred))\n",
    "\n",
    "    print(\"bc Test Macro-F1:\", f1_score(y_test, y_pred, average=\"macro\"))\n",
    "    \n",
    "    return bc_accuracy, bc_macro_f1\n",
    "\n",
    "# For Coffee\n",
    "\n",
    "def coffee_classification_model(train_features, train_target, test_features, test_target):\n",
    "    \n",
    "    X_train = train_features\n",
    "    \n",
    "    y_train = train_target\n",
    "    \n",
    "    X_test = test_features\n",
    "    \n",
    "    y_test = test_target\n",
    "\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), ('bc', BaggingClassifier(estimator = LogisticRegression(max_iter = 1800)))])\n",
    "    \n",
    "    param_grid = {\n",
    "        \n",
    "    'bc__n_estimators': [100, 200],\n",
    "        \n",
    "    'bc__max_samples': [1.0, 0.8, 0.6],\n",
    "        \n",
    "    'bc__random_state': [14]\n",
    "        \n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(pipe, param_grid, cv = 3, scoring = 'accuracy', verbose = 1, n_jobs = -1)\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    print(f\"Best cross-validation accuracy: {grid_search.best_score_}\")\n",
    "    \n",
    "    y_pred = grid_search.predict(X_test)\n",
    "\n",
    "    bc_accuracy = round(100 * (accuracy_score(y_test, y_pred)), 3)\n",
    "\n",
    "    bc_macro_f1 = round(100 * (f1_score(y_test, y_pred, average=\"macro\")), 3)\n",
    "    \n",
    "    print('bc Test accuracy:', accuracy_score(y_test, y_pred))\n",
    "\n",
    "    print(\"bc Test Macro-F1:\", f1_score(y_test, y_pred, average=\"macro\"))\n",
    "    \n",
    "    return bc_accuracy, bc_macro_f1\n",
    "\n",
    "# For Ham\n",
    "\n",
    "def ham_classification_model(train_features, train_target, test_features, test_target):\n",
    "    \n",
    "    X_train = train_features\n",
    "    \n",
    "    y_train = train_target\n",
    "    \n",
    "    X_test = test_features\n",
    "    \n",
    "    y_test = test_target\n",
    "\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), ('bc', BaggingClassifier(estimator = LogisticRegression(max_iter = 2000, solver = 'lbfgs')))])\n",
    "    \n",
    "    param_grid = {\n",
    "        \n",
    "    'bc__n_estimators': [10, 20, 30, 50, 100],\n",
    "        \n",
    "    'bc__max_samples': [1.0, 0.8, 0.6],\n",
    "        \n",
    "    'bc__max_features': [1.0, 0.8],\n",
    "        \n",
    "    'bc__random_state': [45]\n",
    "        \n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(pipe, param_grid, cv = 3, scoring = 'accuracy', verbose = 1, n_jobs = -1)\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    print(f\"Best cross-validation accuracy: {grid_search.best_score_}\")\n",
    "    \n",
    "    y_pred = grid_search.predict(X_test)\n",
    "\n",
    "    bc_accuracy = round(100 * (accuracy_score(y_test, y_pred)), 3)\n",
    "\n",
    "    bc_macro_f1 = round(100 * (f1_score(y_test, y_pred, average=\"macro\")), 3)\n",
    "    \n",
    "    print('bc Test accuracy:', accuracy_score(y_test, y_pred))\n",
    "\n",
    "    print(\"bc Test Macro-F1:\", f1_score(y_test, y_pred, average=\"macro\"))\n",
    "    \n",
    "    return bc_accuracy, bc_macro_f1\n",
    "\n",
    "# For Meat\n",
    "\n",
    "def meat_classification_model(train_features, train_target, test_features, test_target):\n",
    "    \n",
    "    X_train = train_features\n",
    "    \n",
    "    y_train = train_target\n",
    "    \n",
    "    X_test = test_features\n",
    "    \n",
    "    y_test = test_target\n",
    "\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), ('bc', BaggingClassifier(estimator = LogisticRegression(max_iter = 2000, C = 0.01, solver = 'sag')))])\n",
    "    \n",
    "    param_grid = {\n",
    "        \n",
    "    'bc__n_estimators': [10, 20, 30, 50, 100],\n",
    "        \n",
    "    'bc__max_samples': [1.0, 0.8, 0.6],\n",
    "        \n",
    "    'bc__max_features': [1.0, 0.8],\n",
    "        \n",
    "    'bc__random_state': [42]\n",
    "        \n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(pipe, param_grid, cv = 3, scoring = 'accuracy', verbose = 1, n_jobs = -1)\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    print(f\"Best cross-validation accuracy: {grid_search.best_score_}\")\n",
    "    \n",
    "    y_pred = grid_search.predict(X_test)\n",
    "\n",
    "    bc_accuracy = round(100 * (accuracy_score(y_test, y_pred)), 3)\n",
    "\n",
    "    bc_macro_f1 = round(100 * (f1_score(y_test, y_pred, average=\"macro\")), 3)\n",
    "    \n",
    "    print('bc Test accuracy:', accuracy_score(y_test, y_pred))\n",
    "\n",
    "    print(\"bc Test Macro-F1:\", f1_score(y_test, y_pred, average=\"macro\"))\n",
    "    \n",
    "    return bc_accuracy, bc_macro_f1\n",
    "\n",
    "# For Olive oil\n",
    "\n",
    "def oliveoil_classification_model(train_features, train_target, test_features, test_target):\n",
    "    \n",
    "    X_train = train_features\n",
    "    \n",
    "    y_train = train_target\n",
    "    \n",
    "    X_test = test_features\n",
    "    \n",
    "    y_test = test_target\n",
    "\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), ('bc', BaggingClassifier(estimator = LogisticRegression(max_iter = 3500, C = 0.01, solver = 'sag')))])\n",
    "    \n",
    "    param_grid = {\n",
    "        \n",
    "    'bc__n_estimators': [10, 20, 30, 50, 100],\n",
    "        \n",
    "    'bc__max_samples': [1.0, 0.8, 0.6],\n",
    "        \n",
    "    'bc__max_features': [1.0, 0.8],\n",
    "        \n",
    "    'bc__random_state': [14]\n",
    "        \n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(pipe, param_grid, cv = 3, scoring = 'accuracy', verbose = 1, n_jobs = -1)\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    print(f\"Best cross-validation accuracy: {grid_search.best_score_}\")\n",
    "    \n",
    "    y_pred = grid_search.predict(X_test)\n",
    "\n",
    "    bc_accuracy = round(100 * (accuracy_score(y_test, y_pred)), 3)\n",
    "\n",
    "    bc_macro_f1 = round(100 * (f1_score(y_test, y_pred, average=\"macro\")), 3)\n",
    "    \n",
    "    print('bc Test accuracy:', accuracy_score(y_test, y_pred))\n",
    "\n",
    "    print(\"bc Test Macro-F1:\", f1_score(y_test, y_pred, average=\"macro\"))\n",
    "    \n",
    "    return bc_accuracy, bc_macro_f1\n",
    "    \n",
    "# For Strawberry \n",
    "\n",
    "def strawberry_classification_model(train_features, train_target, test_features, test_target):\n",
    "    \n",
    "    X_train = train_features\n",
    "    \n",
    "    y_train = train_target\n",
    "    \n",
    "    X_test = test_features\n",
    "    \n",
    "    y_test = test_target\n",
    "\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), ('bc', BaggingClassifier(estimator = KNeighborsClassifier(n_neighbors = 1, algorithm = 'ball_tree', leaf_size = 20, p = 1)))])\n",
    "    \n",
    "    param_grid = {\n",
    "        \n",
    "    'bc__n_estimators': [10, 20, 30, 50, 100],\n",
    "        \n",
    "    'bc__max_samples': [1.0, 0.8, 0.6],\n",
    "        \n",
    "    'bc__max_features': [1.0, 0.8],\n",
    "        \n",
    "    'bc__random_state': [14]\n",
    "        \n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(pipe, param_grid, cv = 3, scoring = 'accuracy', verbose = 1, n_jobs = -1)\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    print(f\"Best cross-validation accuracy: {grid_search.best_score_}\")\n",
    "    \n",
    "    y_pred = grid_search.predict(X_test)\n",
    "\n",
    "    bc_accuracy = round(100 * (accuracy_score(y_test, y_pred)), 3)\n",
    "\n",
    "    bc_macro_f1 = round(100 * (f1_score(y_test, y_pred, average=\"macro\")), 3)\n",
    "    \n",
    "    print('bc Test accuracy:', accuracy_score(y_test, y_pred))\n",
    "\n",
    "    print(\"bc Test Macro-F1:\", f1_score(y_test, y_pred, average=\"macro\"))\n",
    "    \n",
    "    return bc_accuracy, bc_macro_f1\n",
    "\n",
    "# For Wine\n",
    "\n",
    "def wine_classification_model(train_features, train_target, test_features, test_target):\n",
    "    \n",
    "    X_train = train_features\n",
    "    \n",
    "    y_train = train_target\n",
    "    \n",
    "    X_test = test_features\n",
    "    \n",
    "    y_test = test_target\n",
    "\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), ('bc', BaggingClassifier(estimator = LogisticRegression(max_iter = 1800)))])\n",
    "    \n",
    "    param_grid = {\n",
    "        \n",
    "    'bc__n_estimators': [10, 30, 50, 100],\n",
    "        \n",
    "    'bc__max_samples': [1.0, 0.8, 0.6],\n",
    "        \n",
    "    'bc__random_state': [3]\n",
    "        \n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(pipe, param_grid, cv = 3, scoring = 'accuracy', verbose = 1, n_jobs = -1)\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    print(f\"Best cross-validation accuracy: {grid_search.best_score_}\")\n",
    "    \n",
    "    y_pred = grid_search.predict(X_test)\n",
    "\n",
    "    bc_accuracy = round(100 * (accuracy_score(y_test, y_pred)), 3)\n",
    "\n",
    "    bc_macro_f1 = round(100 * (f1_score(y_test, y_pred, average=\"macro\")), 3)\n",
    "    \n",
    "    print('bc Test accuracy:', accuracy_score(y_test, y_pred))\n",
    "\n",
    "    print(\"bc Test Macro-F1:\", f1_score(y_test, y_pred, average=\"macro\"))\n",
    "    \n",
    "    return bc_accuracy, bc_macro_f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef42568-4be7-40bd-bfbf-5066d2478cf7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## （5）Convert Series to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0621691-dd85-4067-9171-003e5ae8c5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pd.Series([[1,2,3],[4,5,6]]) to pd.DataFrame([[1,2,3],[4,5,6]])\n",
    "\n",
    "def pd_series_to_dataframe(series):\n",
    "    \n",
    "    return pd.DataFrame.from_dict(dict(zip(series.index, series.values))).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce2ade5-1019-4f49-a2df-451eec39223f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## （6）Main Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "22a88260-cce5-41fc-a483-7c5f79b7445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# beef main experiment\n",
    "\n",
    "def beef_experiment(embedding_dimension = 2):\n",
    "\n",
    "    all_classification_accuracy = []   # Store all accuracy results.\n",
    "    all_classification_macro_f1 = []   # Store all macro-f1 results.\n",
    "    \n",
    "    # Step 1. Call the beef_data() method to obtain the beef dataset and convert it into Series format.\n",
    "    \n",
    "    train_data, test_data = beef_data()\n",
    "\n",
    "    train_data_series = pd.Series(train_data['signal'].iloc[:])\n",
    "    \n",
    "    test_data_series = pd.Series(test_data['signal'].iloc[:])\n",
    "\n",
    "    window_size = [5, 10, 45, 50, 95, 100]        # (5, 10) = small size，(45, 50) = medium size，(95, 100) = large size\n",
    "\n",
    "    # Step 2. Apply a sliding window method to all time series in the dataset.\n",
    "    \n",
    "    for size in window_size:\n",
    "        \n",
    "        train_sliding_windows = train_data_series.map(lambda x: sliding_windows(x, size))\n",
    "        \n",
    "        test_sliding_windows = test_data_series.map(lambda x: sliding_windows(x, size))\n",
    "\n",
    "        # Step 3. Apply time-delay embedding method to each window and obtain a 2-dimensional point cloud.\n",
    "        \n",
    "        train_point_cloud = train_sliding_windows.map(lambda x: [takens_embedding(row, embedding_dimension) for row in x])\n",
    "        \n",
    "        test_point_cloud = test_sliding_windows.map(lambda x: [takens_embedding(row, embedding_dimension) for row in x])\n",
    "\n",
    "        # Step 4. Convert each point cloud into a persistence diagram.\n",
    "        \n",
    "        train_persistence_diagrams = train_point_cloud.map(lambda x: [persistent_homology(row) for row in x])\n",
    "        \n",
    "        test_persistence_diagrams = test_point_cloud.map(lambda x: [persistent_homology(row) for row in x])\n",
    "\n",
    "        # Step 5. Separate the H0 and H1 structures in the persistence diagram.\n",
    "        \n",
    "        train_H0 = train_persistence_diagrams.map(lambda x: [row[0] for row in x])\n",
    "        \n",
    "        train_H1 = train_persistence_diagrams.map(lambda x: [row[1] for row in x])\n",
    "\n",
    "        test_H0 = test_persistence_diagrams.map(lambda x: [row[0] for row in x])\n",
    "        \n",
    "        test_H1 = test_persistence_diagrams.map(lambda x: [row[1] for row in x])\n",
    "\n",
    "        # Step 6. Feature Extraction !\n",
    "\n",
    "        features_train_H0 = train_H0.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "        \n",
    "        features_train_H1 = train_H1.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "\n",
    "        features_test_H0 = test_H0.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "        \n",
    "        features_test_H1 = test_H1.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "\n",
    "        # Step 7. Use the itertools package to merge the features of each window into a 1-dimensional array.\n",
    "        \n",
    "        features_train_H0 = features_train_H0.map(lambda x: list(itertools.chain(*x)))\n",
    "        \n",
    "        features_train_H1 = features_train_H1.map(lambda x: list(itertools.chain(*x)))\n",
    "\n",
    "        features_test_H0 = features_test_H0.map(lambda x: list(itertools.chain(*x)))\n",
    "        \n",
    "        features_test_H1 = features_test_H1.map(lambda x: list(itertools.chain(*x)))\n",
    "\n",
    "        # Step 8. Convert all features of H0 and H1 into a DataFrame format, and merge them.\n",
    "        \n",
    "        features_train_H0 = pd_series_to_dataframe(features_train_H0)\n",
    "        \n",
    "        features_train_H1 = pd_series_to_dataframe(features_train_H1)\n",
    "\n",
    "        features_test_H0 = pd_series_to_dataframe(features_test_H0)\n",
    "        \n",
    "        features_test_H1 = pd_series_to_dataframe(features_test_H1)\n",
    "\n",
    "        features_train = pd.concat([features_train_H0, features_train_H1], axis=1)   \n",
    "        \n",
    "        features_train.columns = np.arange(features_train.shape[1])\n",
    "        \n",
    "        features_test = pd.concat([features_test_H0, features_test_H1], axis=1)\n",
    "        \n",
    "        features_test.columns = np.arange(features_test.shape[1])\n",
    "\n",
    "        # Step 9. Convert all features in each row into a list format.\n",
    "        \n",
    "        train_features = []\n",
    "        \n",
    "        features_train = features_train.values.tolist()\n",
    "        \n",
    "        for row in features_train:\n",
    "            \n",
    "            train_features.append(row)\n",
    "\n",
    "        test_features = []\n",
    "        \n",
    "        features_test = features_test.values.tolist()\n",
    "        \n",
    "        for row in features_test:\n",
    "            \n",
    "            test_features.append(row)\n",
    "\n",
    "        # Step 10. Combine all features into the original DataFrame.\n",
    "        \n",
    "        train_data['features'] = train_features\n",
    "        \n",
    "        test_data['features'] = test_features\n",
    "\n",
    "        # Step 11. Machine Learning\n",
    "        \n",
    "        train_features = np.vstack(train_data['features'].values)\n",
    "        \n",
    "        train_target = train_data['target'].values\n",
    "        \n",
    "        test_features = np.vstack(test_data['features'].values)\n",
    "        \n",
    "        test_target = test_data['target'].values\n",
    "\n",
    "        bc_accuracy, bc_macro_f1 = beef_classification_model(train_features, train_target, test_features, test_target)\n",
    "\n",
    "        # Step 12. Organize the classification results into a DataFrame.\n",
    "        \n",
    "        classification_accuracy = [bc_accuracy]\n",
    "\n",
    "        classification_macor_f1 = [bc_macro_f1]\n",
    "        \n",
    "        all_classification_accuracy.append(classification_accuracy)\n",
    "\n",
    "        all_classification_macro_f1.append(classification_macor_f1)\n",
    "        \n",
    "        print('window size ', size,' execution completed !')\n",
    "\n",
    "        print()\n",
    "\n",
    "    all_classification_accuracy = np.array(all_classification_accuracy)                                      \n",
    "    \n",
    "    all_classification_accuracy = pd.DataFrame(all_classification_accuracy, columns = ['Accuracy'])   \n",
    "    \n",
    "    all_classification_accuracy.index = window_size\n",
    "\n",
    "    all_classification_macro_f1 = np.array(all_classification_macro_f1)                                      \n",
    "    \n",
    "    all_classification_macro_f1 = pd.DataFrame(all_classification_macro_f1, columns = ['Macro-F1'])   \n",
    "    \n",
    "    all_classification_macro_f1.index = window_size\n",
    " \n",
    "    return all_classification_accuracy, all_classification_macro_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb1be63a-621f-454f-a004-64c6a4c416af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coffee main experiment\n",
    "\n",
    "def coffee_experiment(embedding_dimension = 2):\n",
    "\n",
    "    all_classification_accuracy = []   # Store all accuracy results.\n",
    "    all_classification_macro_f1 = []   # Store all macro-f1 results.\n",
    "    \n",
    "    # Step 1. Call the coffee_data() method to obtain the beef dataset and convert it into Series format.\n",
    "    \n",
    "    train_data, test_data = coffee_data()\n",
    "\n",
    "    train_data_series = pd.Series(train_data['signal'].iloc[:])\n",
    "    \n",
    "    test_data_series = pd.Series(test_data['signal'].iloc[:])\n",
    "\n",
    "    window_size = [5, 10, 45, 50, 95, 100]        # (5, 10) = small size，(45, 50) = medium size，(95, 100) = large size\n",
    "\n",
    "    # Step 2. Apply a sliding window method to all time series in the dataset.\n",
    "    \n",
    "    for size in window_size:\n",
    "        \n",
    "        train_sliding_windows = train_data_series.map(lambda x: sliding_windows(x, size))\n",
    "        \n",
    "        test_sliding_windows = test_data_series.map(lambda x: sliding_windows(x, size))\n",
    "\n",
    "        # Step 3. Apply time-delay embedding method to each window and obtain a 2-dimensional point cloud.\n",
    "        \n",
    "        train_point_cloud = train_sliding_windows.map(lambda x: [takens_embedding(row, embedding_dimension) for row in x])\n",
    "        \n",
    "        test_point_cloud = test_sliding_windows.map(lambda x: [takens_embedding(row, embedding_dimension) for row in x])\n",
    "\n",
    "        # Step 4. Convert each point cloud into a persistence diagram.\n",
    "        \n",
    "        train_persistence_diagrams = train_point_cloud.map(lambda x: [persistent_homology(row) for row in x])\n",
    "        \n",
    "        test_persistence_diagrams = test_point_cloud.map(lambda x: [persistent_homology(row) for row in x])\n",
    "\n",
    "        # Step 5. Separate the H0 and H1 structures in the persistence diagram.\n",
    "        \n",
    "        train_H0 = train_persistence_diagrams.map(lambda x: [row[0] for row in x])\n",
    "        \n",
    "        train_H1 = train_persistence_diagrams.map(lambda x: [row[1] for row in x])\n",
    "\n",
    "        test_H0 = test_persistence_diagrams.map(lambda x: [row[0] for row in x])\n",
    "        \n",
    "        test_H1 = test_persistence_diagrams.map(lambda x: [row[1] for row in x])\n",
    "\n",
    "        # Step 6. Feature Extraction !\n",
    "\n",
    "        features_train_H0 = train_H0.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "        \n",
    "        features_train_H1 = train_H1.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "\n",
    "        features_test_H0 = test_H0.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "        \n",
    "        features_test_H1 = test_H1.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "\n",
    "        # Step 7. Use the itertools package to merge the features of each window into a 1-dimensional array.\n",
    "        \n",
    "        features_train_H0 = features_train_H0.map(lambda x: list(itertools.chain(*x)))\n",
    "        \n",
    "        features_train_H1 = features_train_H1.map(lambda x: list(itertools.chain(*x)))\n",
    "\n",
    "        features_test_H0 = features_test_H0.map(lambda x: list(itertools.chain(*x)))\n",
    "        \n",
    "        features_test_H1 = features_test_H1.map(lambda x: list(itertools.chain(*x)))\n",
    "\n",
    "        # Step 8. Convert all features of H0 and H1 into a DataFrame format, and merge them.\n",
    "        \n",
    "        features_train_H0 = pd_series_to_dataframe(features_train_H0)\n",
    "        \n",
    "        features_train_H1 = pd_series_to_dataframe(features_train_H1)\n",
    "\n",
    "        features_test_H0 = pd_series_to_dataframe(features_test_H0)\n",
    "        \n",
    "        features_test_H1 = pd_series_to_dataframe(features_test_H1)\n",
    "\n",
    "        features_train = pd.concat([features_train_H0, features_train_H1], axis=1)   \n",
    "        \n",
    "        features_train.columns = np.arange(features_train.shape[1])\n",
    "        \n",
    "        features_test = pd.concat([features_test_H0, features_test_H1], axis=1)\n",
    "        \n",
    "        features_test.columns = np.arange(features_test.shape[1])\n",
    "\n",
    "        # Step 9. Convert all features in each row into a list format.\n",
    "        \n",
    "        train_features = []\n",
    "        \n",
    "        features_train = features_train.values.tolist()\n",
    "        \n",
    "        for row in features_train:\n",
    "            \n",
    "            train_features.append(row)\n",
    "\n",
    "        test_features = []\n",
    "        \n",
    "        features_test = features_test.values.tolist()\n",
    "        \n",
    "        for row in features_test:\n",
    "            \n",
    "            test_features.append(row)\n",
    "\n",
    "        # Step 10. Combine all features into the original DataFrame.\n",
    "        \n",
    "        train_data['features'] = train_features\n",
    "        \n",
    "        test_data['features'] = test_features\n",
    "\n",
    "        # Step 11. Machine Learning\n",
    "        \n",
    "        train_features = np.vstack(train_data['features'].values)\n",
    "        \n",
    "        train_target = train_data['target'].values\n",
    "        \n",
    "        test_features = np.vstack(test_data['features'].values)\n",
    "        \n",
    "        test_target = test_data['target'].values\n",
    "\n",
    "        bc_accuracy, bc_macro_f1 = coffee_classification_model(train_features, train_target, test_features, test_target)\n",
    "\n",
    "        # Step 12. Organize the classification results into a DataFrame.\n",
    "        \n",
    "        classification_accuracy = [bc_accuracy]\n",
    "\n",
    "        classification_macor_f1 = [bc_macro_f1]\n",
    "        \n",
    "        all_classification_accuracy.append(classification_accuracy)\n",
    "\n",
    "        all_classification_macro_f1.append(classification_macor_f1)\n",
    "        \n",
    "        print('window size ', size,' execution completed !')\n",
    "\n",
    "        print()\n",
    "\n",
    "    all_classification_accuracy = np.array(all_classification_accuracy)                                      \n",
    "    \n",
    "    all_classification_accuracy = pd.DataFrame(all_classification_accuracy, columns = ['Accuracy'])   \n",
    "    \n",
    "    all_classification_accuracy.index = window_size\n",
    "\n",
    "    all_classification_macro_f1 = np.array(all_classification_macro_f1)                                      \n",
    "    \n",
    "    all_classification_macro_f1 = pd.DataFrame(all_classification_macro_f1, columns = ['Macro-F1'])   \n",
    "    \n",
    "    all_classification_macro_f1.index = window_size\n",
    " \n",
    "    return all_classification_accuracy, all_classification_macro_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3525c98b-1a37-4927-912d-406b6182fc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ham main experiment\n",
    "\n",
    "def ham_experiment(embedding_dimension = 2):\n",
    "\n",
    "    all_classification_accuracy = []   # Store all accuracy results.\n",
    "    all_classification_macro_f1 = []   # Store all macro-f1 results.\n",
    "    \n",
    "    # Step 1. Call the ham_data() method to obtain the beef dataset and convert it into Series format.\n",
    "    \n",
    "    train_data, test_data = ham_data()\n",
    "\n",
    "    train_data_series = pd.Series(train_data['signal'].iloc[:])\n",
    "    \n",
    "    test_data_series = pd.Series(test_data['signal'].iloc[:])\n",
    "\n",
    "    window_size = [5, 10, 45, 50, 95, 100]        # (5, 10) = small size，(45, 50) = medium size，(95, 100) = large size\n",
    "\n",
    "    # Step 2. Apply a sliding window method to all time series in the dataset.\n",
    "    \n",
    "    for size in window_size:\n",
    "        \n",
    "        train_sliding_windows = train_data_series.map(lambda x: sliding_windows(x, size))\n",
    "        \n",
    "        test_sliding_windows = test_data_series.map(lambda x: sliding_windows(x, size))\n",
    "\n",
    "        # Step 3. Apply time-delay embedding method to each window and obtain a 2-dimensional point cloud.\n",
    "        \n",
    "        train_point_cloud = train_sliding_windows.map(lambda x: [takens_embedding(row, embedding_dimension) for row in x])\n",
    "        \n",
    "        test_point_cloud = test_sliding_windows.map(lambda x: [takens_embedding(row, embedding_dimension) for row in x])\n",
    "\n",
    "        # Step 4. Convert each point cloud into a persistence diagram.\n",
    "        \n",
    "        train_persistence_diagrams = train_point_cloud.map(lambda x: [persistent_homology(row) for row in x])\n",
    "        \n",
    "        test_persistence_diagrams = test_point_cloud.map(lambda x: [persistent_homology(row) for row in x])\n",
    "\n",
    "        # Step 5. Separate the H0 and H1 structures in the persistence diagram.\n",
    "        \n",
    "        train_H0 = train_persistence_diagrams.map(lambda x: [row[0] for row in x])\n",
    "        \n",
    "        train_H1 = train_persistence_diagrams.map(lambda x: [row[1] for row in x])\n",
    "\n",
    "        test_H0 = test_persistence_diagrams.map(lambda x: [row[0] for row in x])\n",
    "        \n",
    "        test_H1 = test_persistence_diagrams.map(lambda x: [row[1] for row in x])\n",
    "\n",
    "        # Step 6. Feature Extraction !\n",
    "\n",
    "        features_train_H0 = train_H0.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "        \n",
    "        features_train_H1 = train_H1.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "\n",
    "        features_test_H0 = test_H0.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "        \n",
    "        features_test_H1 = test_H1.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "\n",
    "        # Step 7. Use the itertools package to merge the features of each window into a 1-dimensional array.\n",
    "        \n",
    "        features_train_H0 = features_train_H0.map(lambda x: list(itertools.chain(*x)))\n",
    "        \n",
    "        features_train_H1 = features_train_H1.map(lambda x: list(itertools.chain(*x)))\n",
    "\n",
    "        features_test_H0 = features_test_H0.map(lambda x: list(itertools.chain(*x)))\n",
    "        \n",
    "        features_test_H1 = features_test_H1.map(lambda x: list(itertools.chain(*x)))\n",
    "\n",
    "        # Step 8. Convert all features of H0 and H1 into a DataFrame format, and merge them.\n",
    "        \n",
    "        features_train_H0 = pd_series_to_dataframe(features_train_H0)\n",
    "        \n",
    "        features_train_H1 = pd_series_to_dataframe(features_train_H1)\n",
    "\n",
    "        features_test_H0 = pd_series_to_dataframe(features_test_H0)\n",
    "        \n",
    "        features_test_H1 = pd_series_to_dataframe(features_test_H1)\n",
    "\n",
    "        features_train = pd.concat([features_train_H0, features_train_H1], axis=1)  \n",
    "        \n",
    "        features_train.columns = np.arange(features_train.shape[1])\n",
    "        \n",
    "        features_test = pd.concat([features_test_H0, features_test_H1], axis=1)\n",
    "        \n",
    "        features_test.columns = np.arange(features_test.shape[1])\n",
    "\n",
    "        # Step 9. Convert all features in each row into a list format.\n",
    "        \n",
    "        train_features = []\n",
    "        \n",
    "        features_train = features_train.values.tolist()\n",
    "        \n",
    "        for row in features_train:\n",
    "            \n",
    "            train_features.append(row)\n",
    "\n",
    "        test_features = []\n",
    "        \n",
    "        features_test = features_test.values.tolist()\n",
    "        \n",
    "        for row in features_test:\n",
    "            \n",
    "            test_features.append(row)\n",
    "\n",
    "        # Step 10. Combine all features into the original DataFrame.\n",
    "        \n",
    "        train_data['features'] = train_features\n",
    "        \n",
    "        test_data['features'] = test_features\n",
    "\n",
    "        # Step 11. Machine Learning\n",
    "        \n",
    "        train_features = np.vstack(train_data['features'].values)\n",
    "        \n",
    "        train_target = train_data['target'].values\n",
    "        \n",
    "        test_features = np.vstack(test_data['features'].values)\n",
    "        \n",
    "        test_target = test_data['target'].values\n",
    "\n",
    "        bc_accuracy, bc_macro_f1 = ham_classification_model(train_features, train_target, test_features, test_target)\n",
    "\n",
    "        # Step 12. Organize the classification results into a DataFrame.\n",
    "        \n",
    "        classification_accuracy = [bc_accuracy]\n",
    "\n",
    "        classification_macor_f1 = [bc_macro_f1]\n",
    "        \n",
    "        all_classification_accuracy.append(classification_accuracy)\n",
    "\n",
    "        all_classification_macro_f1.append(classification_macor_f1)\n",
    "        \n",
    "        print('window size ', size,' execution completed !')\n",
    "\n",
    "        print()\n",
    "\n",
    "    all_classification_accuracy = np.array(all_classification_accuracy)                                      \n",
    "    \n",
    "    all_classification_accuracy = pd.DataFrame(all_classification_accuracy, columns = ['Accuracy'])   \n",
    "    \n",
    "    all_classification_accuracy.index = window_size\n",
    "\n",
    "    all_classification_macro_f1 = np.array(all_classification_macro_f1)                                      \n",
    "    \n",
    "    all_classification_macro_f1 = pd.DataFrame(all_classification_macro_f1, columns = ['Macro-F1'])   \n",
    "    \n",
    "    all_classification_macro_f1.index = window_size\n",
    " \n",
    "    return all_classification_accuracy, all_classification_macro_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa4558ae-a803-49d9-99eb-7dded921b530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meat main experiment\n",
    "\n",
    "def meat_experiment(embedding_dimension = 2):\n",
    "\n",
    "    all_classification_accuracy = []   # Store all accuracy results.\n",
    "    all_classification_macro_f1 = []   # Store all macro-f1 results.\n",
    "    \n",
    "    # Step 1. Call the meat_data() method to obtain the beef dataset and convert it into Series format.\n",
    "    \n",
    "    train_data, test_data = meat_data()\n",
    "\n",
    "    train_data_series = pd.Series(train_data['signal'].iloc[:])\n",
    "    \n",
    "    test_data_series = pd.Series(test_data['signal'].iloc[:])\n",
    "\n",
    "    window_size = [5, 10, 45, 50, 95, 100]        # (5, 10) = small size，(45, 50) = medium size，(95, 100) = large size\n",
    "\n",
    "    # Step 2. Apply a sliding window method to all time series in the dataset.\n",
    "    \n",
    "    for size in window_size:\n",
    "        \n",
    "        train_sliding_windows = train_data_series.map(lambda x: sliding_windows(x, size))\n",
    "        \n",
    "        test_sliding_windows = test_data_series.map(lambda x: sliding_windows(x, size))\n",
    "\n",
    "        # Step 3. Apply time-delay embedding method to each window and obtain a 2-dimensional point cloud.\n",
    "        \n",
    "        train_point_cloud = train_sliding_windows.map(lambda x: [takens_embedding(row, embedding_dimension) for row in x])\n",
    "        \n",
    "        test_point_cloud = test_sliding_windows.map(lambda x: [takens_embedding(row, embedding_dimension) for row in x])\n",
    "\n",
    "        # Step 4. Convert each point cloud into a persistence diagram.\n",
    "        \n",
    "        train_persistence_diagrams = train_point_cloud.map(lambda x: [persistent_homology(row) for row in x])\n",
    "        \n",
    "        test_persistence_diagrams = test_point_cloud.map(lambda x: [persistent_homology(row) for row in x])\n",
    "\n",
    "        # Step 5. Separate the H0 and H1 structures in the persistence diagram.\n",
    "        \n",
    "        train_H0 = train_persistence_diagrams.map(lambda x: [row[0] for row in x])\n",
    "        \n",
    "        train_H1 = train_persistence_diagrams.map(lambda x: [row[1] for row in x])\n",
    "\n",
    "        test_H0 = test_persistence_diagrams.map(lambda x: [row[0] for row in x])\n",
    "        \n",
    "        test_H1 = test_persistence_diagrams.map(lambda x: [row[1] for row in x])\n",
    "\n",
    "        # Step 6. Feature Extraction !\n",
    "\n",
    "        features_train_H0 = train_H0.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "        \n",
    "        features_train_H1 = train_H1.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "\n",
    "        features_test_H0 = test_H0.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "        \n",
    "        features_test_H1 = test_H1.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "\n",
    "        # Step 7. Use the itertools package to merge the features of each window into a 1-dimensional array.\n",
    "        \n",
    "        features_train_H0 = features_train_H0.map(lambda x: list(itertools.chain(*x)))\n",
    "        \n",
    "        features_train_H1 = features_train_H1.map(lambda x: list(itertools.chain(*x)))\n",
    "\n",
    "        features_test_H0 = features_test_H0.map(lambda x: list(itertools.chain(*x)))\n",
    "        \n",
    "        features_test_H1 = features_test_H1.map(lambda x: list(itertools.chain(*x)))\n",
    "\n",
    "        # Step 8. Convert all features of H0 and H1 into a DataFrame format, and merge them.\n",
    "        \n",
    "        features_train_H0 = pd_series_to_dataframe(features_train_H0)\n",
    "        \n",
    "        features_train_H1 = pd_series_to_dataframe(features_train_H1)\n",
    "\n",
    "        features_test_H0 = pd_series_to_dataframe(features_test_H0)\n",
    "        \n",
    "        features_test_H1 = pd_series_to_dataframe(features_test_H1)\n",
    "\n",
    "        features_train = pd.concat([features_train_H0, features_train_H1], axis=1)   \n",
    "        \n",
    "        features_train.columns = np.arange(features_train.shape[1])\n",
    "        \n",
    "        features_test = pd.concat([features_test_H0, features_test_H1], axis=1)\n",
    "        \n",
    "        features_test.columns = np.arange(features_test.shape[1])\n",
    "\n",
    "        # Step 9. Convert all features in each row into a list format.\n",
    "        \n",
    "        train_features = []\n",
    "        \n",
    "        features_train = features_train.values.tolist()\n",
    "        \n",
    "        for row in features_train:\n",
    "            \n",
    "            train_features.append(row)\n",
    "\n",
    "        test_features = []\n",
    "        \n",
    "        features_test = features_test.values.tolist()\n",
    "        \n",
    "        for row in features_test:\n",
    "            \n",
    "            test_features.append(row)\n",
    "\n",
    "        # Step 10. Combine all features into the original DataFrame.\n",
    "        \n",
    "        train_data['features'] = train_features\n",
    "        \n",
    "        test_data['features'] = test_features\n",
    "\n",
    "        # Step 11. Machine Learning\n",
    "        \n",
    "        train_features = np.vstack(train_data['features'].values)\n",
    "        \n",
    "        train_target = train_data['target'].values\n",
    "        \n",
    "        test_features = np.vstack(test_data['features'].values)\n",
    "        \n",
    "        test_target = test_data['target'].values\n",
    "\n",
    "        bc_accuracy, bc_macro_f1 = meat_classification_model(train_features, train_target, test_features, test_target)\n",
    "\n",
    "        # Step 12. Organize the classification results into a DataFrame.\n",
    "        \n",
    "        classification_accuracy = [bc_accuracy]\n",
    "\n",
    "        classification_macor_f1 = [bc_macro_f1]\n",
    "        \n",
    "        all_classification_accuracy.append(classification_accuracy)\n",
    "\n",
    "        all_classification_macro_f1.append(classification_macor_f1)\n",
    "        \n",
    "        print('window size ', size,' execution completed !')\n",
    "\n",
    "        print()\n",
    "\n",
    "    all_classification_accuracy = np.array(all_classification_accuracy)                                      \n",
    "    \n",
    "    all_classification_accuracy = pd.DataFrame(all_classification_accuracy, columns = ['Accuracy'])   \n",
    "    \n",
    "    all_classification_accuracy.index = window_size\n",
    "\n",
    "    all_classification_macro_f1 = np.array(all_classification_macro_f1)                                      \n",
    "    \n",
    "    all_classification_macro_f1 = pd.DataFrame(all_classification_macro_f1, columns = ['Macro-F1'])   \n",
    "    \n",
    "    all_classification_macro_f1.index = window_size\n",
    " \n",
    "    return all_classification_accuracy, all_classification_macro_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "18745075-f7a3-4f14-a54f-59284370e2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# olive oil main experiment\n",
    "\n",
    "def oliveoil_experiment(embedding_dimension = 2):\n",
    "\n",
    "    all_classification_accuracy = []   # Store all accuracy results.\n",
    "    all_classification_macro_f1 = []   # Store all macro-f1 results.\n",
    "    \n",
    "    # Step 1. Call the oliveoil_data() method to obtain the beef dataset and convert it into Series format.\n",
    "    \n",
    "    train_data, test_data = oliveoil_data()\n",
    "\n",
    "    train_data_series = pd.Series(train_data['signal'].iloc[:])\n",
    "    \n",
    "    test_data_series = pd.Series(test_data['signal'].iloc[:])\n",
    "\n",
    "    window_size = [5, 10, 45, 50, 95, 100]        # (5, 10) = small size，(45, 50) = medium size，(95, 100) = large size\n",
    "\n",
    "    # Step 2. Apply a sliding window method to all time series in the dataset.\n",
    "    \n",
    "    for size in window_size:\n",
    "        \n",
    "        train_sliding_windows = train_data_series.map(lambda x: sliding_windows(x, size))\n",
    "        \n",
    "        test_sliding_windows = test_data_series.map(lambda x: sliding_windows(x, size))\n",
    "\n",
    "        # Step 3. Apply time-delay embedding method to each window and obtain a 2-dimensional point cloud.\n",
    "        \n",
    "        train_point_cloud = train_sliding_windows.map(lambda x: [takens_embedding(row, embedding_dimension) for row in x])\n",
    "        \n",
    "        test_point_cloud = test_sliding_windows.map(lambda x: [takens_embedding(row, embedding_dimension) for row in x])\n",
    "\n",
    "        # Step 4. Convert each point cloud into a persistence diagram.\n",
    "        \n",
    "        train_persistence_diagrams = train_point_cloud.map(lambda x: [persistent_homology(row) for row in x])\n",
    "        \n",
    "        test_persistence_diagrams = test_point_cloud.map(lambda x: [persistent_homology(row) for row in x])\n",
    "\n",
    "        # Step 5. Separate the H0 and H1 structures in the persistence diagram.\n",
    "        \n",
    "        train_H0 = train_persistence_diagrams.map(lambda x: [row[0] for row in x])\n",
    "        \n",
    "        train_H1 = train_persistence_diagrams.map(lambda x: [row[1] for row in x])\n",
    "\n",
    "        test_H0 = test_persistence_diagrams.map(lambda x: [row[0] for row in x])\n",
    "        \n",
    "        test_H1 = test_persistence_diagrams.map(lambda x: [row[1] for row in x])\n",
    "\n",
    "        # Step 6. Feature Extraction !\n",
    "\n",
    "        features_train_H0 = train_H0.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "        \n",
    "        features_train_H1 = train_H1.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "\n",
    "        features_test_H0 = test_H0.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "        \n",
    "        features_test_H1 = test_H1.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "\n",
    "        # Step 7. Use the itertools package to merge the features of each window into a 1-dimensional array.\n",
    "        \n",
    "        features_train_H0 = features_train_H0.map(lambda x: list(itertools.chain(*x)))\n",
    "        \n",
    "        features_train_H1 = features_train_H1.map(lambda x: list(itertools.chain(*x)))\n",
    "\n",
    "        features_test_H0 = features_test_H0.map(lambda x: list(itertools.chain(*x)))\n",
    "        \n",
    "        features_test_H1 = features_test_H1.map(lambda x: list(itertools.chain(*x)))\n",
    "\n",
    "        # Step 8. Convert all features of H0 and H1 into a DataFrame format, and merge them.\n",
    "        \n",
    "        features_train_H0 = pd_series_to_dataframe(features_train_H0)\n",
    "        \n",
    "        features_train_H1 = pd_series_to_dataframe(features_train_H1)\n",
    "\n",
    "        features_test_H0 = pd_series_to_dataframe(features_test_H0)\n",
    "        \n",
    "        features_test_H1 = pd_series_to_dataframe(features_test_H1)\n",
    "\n",
    "        features_train = pd.concat([features_train_H0, features_train_H1], axis=1)   \n",
    "        \n",
    "        features_train.columns = np.arange(features_train.shape[1])\n",
    "        \n",
    "        features_test = pd.concat([features_test_H0, features_test_H1], axis=1)\n",
    "        \n",
    "        features_test.columns = np.arange(features_test.shape[1])\n",
    "\n",
    "        # Step 9. Convert all features in each row into a list format.\n",
    "        \n",
    "        train_features = []\n",
    "        \n",
    "        features_train = features_train.values.tolist()\n",
    "        \n",
    "        for row in features_train:\n",
    "            \n",
    "            train_features.append(row)\n",
    "\n",
    "        test_features = []\n",
    "        \n",
    "        features_test = features_test.values.tolist()\n",
    "        \n",
    "        for row in features_test:\n",
    "            \n",
    "            test_features.append(row)\n",
    "\n",
    "        # Step 10. Combine all features into the original DataFrame.\n",
    "        \n",
    "        train_data['features'] = train_features\n",
    "        \n",
    "        test_data['features'] = test_features\n",
    "\n",
    "        # Step 11. Machine Learning\n",
    "        \n",
    "        train_features = np.vstack(train_data['features'].values)\n",
    "        \n",
    "        train_target = train_data['target'].values\n",
    "        \n",
    "        test_features = np.vstack(test_data['features'].values)\n",
    "        \n",
    "        test_target = test_data['target'].values\n",
    "\n",
    "        bc_accuracy, bc_macro_f1 = oliveoil_classification_model(train_features, train_target, test_features, test_target)\n",
    "\n",
    "        # Step 12. Organize the classification results into a DataFrame.\n",
    "        \n",
    "        classification_accuracy = [bc_accuracy]\n",
    "\n",
    "        classification_macor_f1 = [bc_macro_f1]\n",
    "        \n",
    "        all_classification_accuracy.append(classification_accuracy)\n",
    "\n",
    "        all_classification_macro_f1.append(classification_macor_f1)\n",
    "        \n",
    "        print('window size ', size,' execution completed !')\n",
    "\n",
    "        print()\n",
    "\n",
    "    all_classification_accuracy = np.array(all_classification_accuracy)                                      \n",
    "    \n",
    "    all_classification_accuracy = pd.DataFrame(all_classification_accuracy, columns = ['Accuracy'])   \n",
    "    \n",
    "    all_classification_accuracy.index = window_size\n",
    "\n",
    "    all_classification_macro_f1 = np.array(all_classification_macro_f1)                                      \n",
    "    \n",
    "    all_classification_macro_f1 = pd.DataFrame(all_classification_macro_f1, columns = ['Macro-F1'])   \n",
    "    \n",
    "    all_classification_macro_f1.index = window_size\n",
    " \n",
    "    return all_classification_accuracy, all_classification_macro_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a831997-4c38-4d2c-b3d8-b42fc0771c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# strawberry main experiment\n",
    "\n",
    "def strawberry_experiment(embedding_dimension = 2):\n",
    "\n",
    "    all_classification_accuracy = []   # Store all accuracy results.\n",
    "    all_classification_macro_f1 = []   # Store all macro-f1 results.\n",
    "    \n",
    "    # Step 1. Call the strawberry_data() method to obtain the beef dataset and convert it into Series format.\n",
    "    \n",
    "    train_data, test_data = strawberry_data()\n",
    "\n",
    "    train_data_series = pd.Series(train_data['signal'].iloc[:])\n",
    "    \n",
    "    test_data_series = pd.Series(test_data['signal'].iloc[:])\n",
    "\n",
    "    window_size = [5, 10, 45, 50, 95, 100]        # (5, 10) = small size，(45, 50) = medium size，(95, 100) = large size\n",
    "\n",
    "    # Step 2. Apply a sliding window method to all time series in the dataset.\n",
    "    \n",
    "    for size in window_size:\n",
    "        \n",
    "        train_sliding_windows = train_data_series.map(lambda x: sliding_windows(x, size))\n",
    "        \n",
    "        test_sliding_windows = test_data_series.map(lambda x: sliding_windows(x, size))\n",
    "\n",
    "        # Step 3. Apply time-delay embedding method to each window and obtain a 2-dimensional point cloud.\n",
    "        \n",
    "        train_point_cloud = train_sliding_windows.map(lambda x: [takens_embedding(row, embedding_dimension) for row in x])\n",
    "        \n",
    "        test_point_cloud = test_sliding_windows.map(lambda x: [takens_embedding(row, embedding_dimension) for row in x])\n",
    "\n",
    "        # Step 4. Convert each point cloud into a persistence diagram.\n",
    "        \n",
    "        train_persistence_diagrams = train_point_cloud.map(lambda x: [persistent_homology(row) for row in x])\n",
    "        \n",
    "        test_persistence_diagrams = test_point_cloud.map(lambda x: [persistent_homology(row) for row in x])\n",
    "\n",
    "        # Step 5. Separate the H0 and H1 structures in the persistence diagram.\n",
    "        \n",
    "        train_H0 = train_persistence_diagrams.map(lambda x: [row[0] for row in x])\n",
    "        \n",
    "        train_H1 = train_persistence_diagrams.map(lambda x: [row[1] for row in x])\n",
    "\n",
    "        test_H0 = test_persistence_diagrams.map(lambda x: [row[0] for row in x])\n",
    "        \n",
    "        test_H1 = test_persistence_diagrams.map(lambda x: [row[1] for row in x])\n",
    "\n",
    "        # Step 6. Feature Extraction !\n",
    "\n",
    "        features_train_H0 = train_H0.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "        \n",
    "        features_train_H1 = train_H1.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "\n",
    "        features_test_H0 = test_H0.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "        \n",
    "        features_test_H1 = test_H1.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "\n",
    "        # Step 7. Use the itertools package to merge the features of each window into a 1-dimensional array.\n",
    "        \n",
    "        features_train_H0 = features_train_H0.map(lambda x: list(itertools.chain(*x)))\n",
    "        \n",
    "        features_train_H1 = features_train_H1.map(lambda x: list(itertools.chain(*x)))\n",
    "\n",
    "        features_test_H0 = features_test_H0.map(lambda x: list(itertools.chain(*x)))\n",
    "        \n",
    "        features_test_H1 = features_test_H1.map(lambda x: list(itertools.chain(*x)))\n",
    "\n",
    "        # Step 8. Convert all features of H0 and H1 into a DataFrame format, and merge them.\n",
    "        \n",
    "        features_train_H0 = pd_series_to_dataframe(features_train_H0)\n",
    "        \n",
    "        features_train_H1 = pd_series_to_dataframe(features_train_H1)\n",
    "\n",
    "        features_test_H0 = pd_series_to_dataframe(features_test_H0)\n",
    "        \n",
    "        features_test_H1 = pd_series_to_dataframe(features_test_H1)\n",
    "\n",
    "        features_train = pd.concat([features_train_H0, features_train_H1], axis=1)   \n",
    "        \n",
    "        features_train.columns = np.arange(features_train.shape[1])\n",
    "        \n",
    "        features_test = pd.concat([features_test_H0, features_test_H1], axis=1)\n",
    "        \n",
    "        features_test.columns = np.arange(features_test.shape[1])\n",
    "\n",
    "        # Step 9. Convert all features in each row into a list format.\n",
    "        \n",
    "        train_features = []\n",
    "        \n",
    "        features_train = features_train.values.tolist()\n",
    "        \n",
    "        for row in features_train:\n",
    "            \n",
    "            train_features.append(row)\n",
    "\n",
    "        test_features = []\n",
    "        \n",
    "        features_test = features_test.values.tolist()\n",
    "        \n",
    "        for row in features_test:\n",
    "            \n",
    "            test_features.append(row)\n",
    "\n",
    "        # Step 10. Combine all features into the original DataFrame.\n",
    "        \n",
    "        train_data['features'] = train_features\n",
    "        \n",
    "        test_data['features'] = test_features\n",
    "\n",
    "        # Step 11. Machine Learning\n",
    "        \n",
    "        train_features = np.vstack(train_data['features'].values)\n",
    "        \n",
    "        train_target = train_data['target'].values\n",
    "        \n",
    "        test_features = np.vstack(test_data['features'].values)\n",
    "        \n",
    "        test_target = test_data['target'].values\n",
    "\n",
    "        bc_accuracy, bc_macro_f1 = strawberry_classification_model(train_features, train_target, test_features, test_target)\n",
    "\n",
    "        # Step 12. Organize the classification results into a DataFrame.\n",
    "        \n",
    "        classification_accuracy = [bc_accuracy]\n",
    "\n",
    "        classification_macor_f1 = [bc_macro_f1]\n",
    "        \n",
    "        all_classification_accuracy.append(classification_accuracy)\n",
    "\n",
    "        all_classification_macro_f1.append(classification_macor_f1)\n",
    "        \n",
    "        print('window size ', size,' execution completed !')\n",
    "\n",
    "        print()\n",
    "\n",
    "    all_classification_accuracy = np.array(all_classification_accuracy)                                      \n",
    "    \n",
    "    all_classification_accuracy = pd.DataFrame(all_classification_accuracy, columns = ['Accuracy'])   \n",
    "    \n",
    "    all_classification_accuracy.index = window_size\n",
    "\n",
    "    all_classification_macro_f1 = np.array(all_classification_macro_f1)                                      \n",
    "    \n",
    "    all_classification_macro_f1 = pd.DataFrame(all_classification_macro_f1, columns = ['Macro-F1'])   \n",
    "    \n",
    "    all_classification_macro_f1.index = window_size\n",
    " \n",
    "    return all_classification_accuracy, all_classification_macro_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c4fd0d46-0436-4b42-847a-62f432cf2c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wine main experiment\n",
    "\n",
    "def wine_experiment(embedding_dimension = 2):\n",
    "\n",
    "    all_classification_accuracy = []   # Store all accuracy results.\n",
    "    all_classification_macro_f1 = []   # Store all macro-f1 results.\n",
    "    \n",
    "    # Step 1. Call the wine_data() method to obtain the beef dataset and convert it into Series format.\n",
    "    \n",
    "    train_data, test_data = wine_data()\n",
    "\n",
    "    train_data_series = pd.Series(train_data['signal'].iloc[:])\n",
    "    \n",
    "    test_data_series = pd.Series(test_data['signal'].iloc[:])\n",
    "\n",
    "    window_size = [5, 10, 45, 50, 95, 100]        # (5, 10) = small size，(45, 50) = medium size，(95, 100) = large size\n",
    "\n",
    "    # Step 2. Apply a sliding window method to all time series in the dataset.\n",
    "    \n",
    "    for size in window_size:\n",
    "        \n",
    "        train_sliding_windows = train_data_series.map(lambda x: sliding_windows(x, size))\n",
    "        \n",
    "        test_sliding_windows = test_data_series.map(lambda x: sliding_windows(x, size))\n",
    "\n",
    "        # Step 3. Apply time-delay embedding method to each window and obtain a 2-dimensional point cloud.\n",
    "        \n",
    "        train_point_cloud = train_sliding_windows.map(lambda x: [takens_embedding(row, embedding_dimension) for row in x])\n",
    "        \n",
    "        test_point_cloud = test_sliding_windows.map(lambda x: [takens_embedding(row, embedding_dimension) for row in x])\n",
    "\n",
    "        # Step 4. Convert each point cloud into a persistence diagram.\n",
    "        \n",
    "        train_persistence_diagrams = train_point_cloud.map(lambda x: [persistent_homology(row) for row in x])\n",
    "        \n",
    "        test_persistence_diagrams = test_point_cloud.map(lambda x: [persistent_homology(row) for row in x])\n",
    "\n",
    "        # Step 5. Separate the H0 and H1 structures in the persistence diagram.\n",
    "        \n",
    "        train_H0 = train_persistence_diagrams.map(lambda x: [row[0] for row in x])\n",
    "        \n",
    "        train_H1 = train_persistence_diagrams.map(lambda x: [row[1] for row in x])\n",
    "\n",
    "        test_H0 = test_persistence_diagrams.map(lambda x: [row[0] for row in x])\n",
    "        \n",
    "        test_H1 = test_persistence_diagrams.map(lambda x: [row[1] for row in x])\n",
    "\n",
    "        # Step 6. Feature Extraction !\n",
    "\n",
    "        features_train_H0 = train_H0.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "        \n",
    "        features_train_H1 = train_H1.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "\n",
    "        features_test_H0 = test_H0.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "        \n",
    "        features_test_H1 = test_H1.map(lambda x: [get_diagram_features(row) for row in x])\n",
    "\n",
    "        # Step 7. Use the itertools package to merge the features of each window into a 1-dimensional array.\n",
    "        \n",
    "        features_train_H0 = features_train_H0.map(lambda x: list(itertools.chain(*x)))\n",
    "        \n",
    "        features_train_H1 = features_train_H1.map(lambda x: list(itertools.chain(*x)))\n",
    "\n",
    "        features_test_H0 = features_test_H0.map(lambda x: list(itertools.chain(*x)))\n",
    "        \n",
    "        features_test_H1 = features_test_H1.map(lambda x: list(itertools.chain(*x)))\n",
    "\n",
    "        # Step 8. Convert all features of H0 and H1 into a DataFrame format, and merge them.\n",
    "        \n",
    "        features_train_H0 = pd_series_to_dataframe(features_train_H0)\n",
    "        \n",
    "        features_train_H1 = pd_series_to_dataframe(features_train_H1)\n",
    "\n",
    "        features_test_H0 = pd_series_to_dataframe(features_test_H0)\n",
    "        \n",
    "        features_test_H1 = pd_series_to_dataframe(features_test_H1)\n",
    "\n",
    "        features_train = pd.concat([features_train_H0, features_train_H1], axis=1)   \n",
    "        \n",
    "        features_train.columns = np.arange(features_train.shape[1])\n",
    "        \n",
    "        features_test = pd.concat([features_test_H0, features_test_H1], axis=1)\n",
    "        \n",
    "        features_test.columns = np.arange(features_test.shape[1])\n",
    "\n",
    "        # Step 9. Convert all features in each row into a list format.\n",
    "        \n",
    "        train_features = []\n",
    "        \n",
    "        features_train = features_train.values.tolist()\n",
    "        \n",
    "        for row in features_train:\n",
    "            \n",
    "            train_features.append(row)\n",
    "\n",
    "        test_features = []\n",
    "        \n",
    "        features_test = features_test.values.tolist()\n",
    "        \n",
    "        for row in features_test:\n",
    "            \n",
    "            test_features.append(row)\n",
    "\n",
    "        # Step 10. Combine all features into the original DataFrame.\n",
    "        \n",
    "        train_data['features'] = train_features\n",
    "        \n",
    "        test_data['features'] = test_features\n",
    "\n",
    "        # Step 11. Machine Learning\n",
    "        \n",
    "        train_features = np.vstack(train_data['features'].values)\n",
    "        \n",
    "        train_target = train_data['target'].values\n",
    "        \n",
    "        test_features = np.vstack(test_data['features'].values)\n",
    "        \n",
    "        test_target = test_data['target'].values\n",
    "\n",
    "        bc_accuracy, bc_macro_f1 = wine_classification_model(train_features, train_target, test_features, test_target)\n",
    "\n",
    "        # Step 12. Organize the classification results into a DataFrame.\n",
    "        \n",
    "        classification_accuracy = [bc_accuracy]\n",
    "\n",
    "        classification_macor_f1 = [bc_macro_f1]\n",
    "        \n",
    "        all_classification_accuracy.append(classification_accuracy)\n",
    "\n",
    "        all_classification_macro_f1.append(classification_macor_f1)\n",
    "        \n",
    "        print('window size ', size,' execution completed !')\n",
    "\n",
    "        print()\n",
    "\n",
    "    all_classification_accuracy = np.array(all_classification_accuracy)                                      \n",
    "    \n",
    "    all_classification_accuracy = pd.DataFrame(all_classification_accuracy, columns = ['Accuracy'])   \n",
    "    \n",
    "    all_classification_accuracy.index = window_size\n",
    "\n",
    "    all_classification_macro_f1 = np.array(all_classification_macro_f1)                                      \n",
    "    \n",
    "    all_classification_macro_f1 = pd.DataFrame(all_classification_macro_f1, columns = ['Macro-F1'])   \n",
    "    \n",
    "    all_classification_macro_f1.index = window_size\n",
    " \n",
    "    return all_classification_accuracy, all_classification_macro_f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154190cc-bf19-4af7-ae6e-a0231cf4aae4",
   "metadata": {},
   "source": [
    "## （7）Experimental results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43161698-6e4e-407a-8934-e1c94660d253",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### <font color = green >The classification Accuracy and Macro-F1 score of the Beef dataset</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "80fb4c2f-45e8-4d57-bccd-b071df172c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Best parameters: {'bc__n_estimators': 50, 'bc__random_state': 14}\n",
      "Best cross-validation accuracy: 0.5999999999999999\n",
      "bc Test accuracy: 0.9333333333333333\n",
      "bc Test Macro-F1: 0.9328671328671329\n",
      "window size  5  execution completed !\n",
      "\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Best parameters: {'bc__n_estimators': 50, 'bc__random_state': 14}\n",
      "Best cross-validation accuracy: 0.6666666666666667\n",
      "bc Test accuracy: 0.8666666666666667\n",
      "bc Test Macro-F1: 0.8664935064935065\n",
      "window size  10  execution completed !\n",
      "\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Best parameters: {'bc__n_estimators': 50, 'bc__random_state': 14}\n",
      "Best cross-validation accuracy: 0.5333333333333333\n",
      "bc Test accuracy: 0.7333333333333333\n",
      "bc Test Macro-F1: 0.7321212121212122\n",
      "window size  45  execution completed !\n",
      "\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Best parameters: {'bc__n_estimators': 50, 'bc__random_state': 14}\n",
      "Best cross-validation accuracy: 0.5666666666666667\n",
      "bc Test accuracy: 0.7\n",
      "bc Test Macro-F1: 0.7152380952380952\n",
      "window size  50  execution completed !\n",
      "\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Best parameters: {'bc__n_estimators': 50, 'bc__random_state': 14}\n",
      "Best cross-validation accuracy: 0.3666666666666666\n",
      "bc Test accuracy: 0.5333333333333333\n",
      "bc Test Macro-F1: 0.5321212121212121\n",
      "window size  95  execution completed !\n",
      "\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Best parameters: {'bc__n_estimators': 50, 'bc__random_state': 14}\n",
      "Best cross-validation accuracy: 0.3\n",
      "bc Test accuracy: 0.5\n",
      "bc Test Macro-F1: 0.49757575757575756\n",
      "window size  100  execution completed !\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>93.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>86.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>73.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>70.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>53.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>50.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Accuracy\n",
       "5      93.333\n",
       "10     86.667\n",
       "45     73.333\n",
       "50     70.000\n",
       "95     53.333\n",
       "100    50.000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# beef\n",
    "\n",
    "beef_accuracy, beef_macro_f1 = beef_experiment(embedding_dimension = 2)\n",
    "\n",
    "beef_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fb6199fe-0005-47ae-b368-f345568b6269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Macro-F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>93.287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>86.649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>73.212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>71.524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>53.212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>49.758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Macro-F1\n",
       "5      93.287\n",
       "10     86.649\n",
       "45     73.212\n",
       "50     71.524\n",
       "95     53.212\n",
       "100    49.758"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beef_macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e701d9a-6dbb-450e-8403-b7420098f314",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Beef_Research_result.csv',  'w', encoding = 'utf = 8') as file:\n",
    "    \n",
    "     beef_accuracy.to_csv(file)\n",
    "\n",
    "with open('Beef_MacroF1_result.csv',  'w', encoding = 'utf = 8') as file:\n",
    "    \n",
    "     beef_macro_f1.to_csv(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d86cf1-60f3-464e-afe7-39021d7d2ebd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### <font color = green >The classification Accuracy and Macro-F1 score of the Coffee dataset</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "27f10525-f230-484f-845c-c3c8b20a5dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Best parameters: {'bc__max_samples': 1.0, 'bc__n_estimators': 100, 'bc__random_state': 14}\n",
      "Best cross-validation accuracy: 0.9629629629629629\n",
      "bc Test accuracy: 0.8928571428571429\n",
      "bc Test Macro-F1: 0.8893280632411067\n",
      "window size  5  execution completed !\n",
      "\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Best parameters: {'bc__max_samples': 1.0, 'bc__n_estimators': 100, 'bc__random_state': 14}\n",
      "Best cross-validation accuracy: 1.0\n",
      "bc Test accuracy: 1.0\n",
      "bc Test Macro-F1: 1.0\n",
      "window size  10  execution completed !\n",
      "\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Best parameters: {'bc__max_samples': 1.0, 'bc__n_estimators': 100, 'bc__random_state': 14}\n",
      "Best cross-validation accuracy: 0.9629629629629629\n",
      "bc Test accuracy: 0.9285714285714286\n",
      "bc Test Macro-F1: 0.9282051282051282\n",
      "window size  45  execution completed !\n",
      "\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Best parameters: {'bc__max_samples': 1.0, 'bc__n_estimators': 200, 'bc__random_state': 14}\n",
      "Best cross-validation accuracy: 0.9666666666666667\n",
      "bc Test accuracy: 0.9285714285714286\n",
      "bc Test Macro-F1: 0.9282051282051282\n",
      "window size  50  execution completed !\n",
      "\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Best parameters: {'bc__max_samples': 0.8, 'bc__n_estimators': 200, 'bc__random_state': 14}\n",
      "Best cross-validation accuracy: 0.8629629629629628\n",
      "bc Test accuracy: 1.0\n",
      "bc Test Macro-F1: 1.0\n",
      "window size  95  execution completed !\n",
      "\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Best parameters: {'bc__max_samples': 1.0, 'bc__n_estimators': 100, 'bc__random_state': 14}\n",
      "Best cross-validation accuracy: 0.825925925925926\n",
      "bc Test accuracy: 1.0\n",
      "bc Test Macro-F1: 1.0\n",
      "window size  100  execution completed !\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>89.286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>100.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>92.857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>92.857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>100.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Accuracy\n",
       "5      89.286\n",
       "10    100.000\n",
       "45     92.857\n",
       "50     92.857\n",
       "95    100.000\n",
       "100   100.000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coffee\n",
    "\n",
    "coffee_accuracy, coffee_macro_f1 = coffee_experiment(embedding_dimension = 2)\n",
    "\n",
    "coffee_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fb130cdb-c2c5-432b-a674-67a2404091f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Macro-F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>88.933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>100.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>92.821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>92.821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>100.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Macro-F1\n",
       "5      88.933\n",
       "10    100.000\n",
       "45     92.821\n",
       "50     92.821\n",
       "95    100.000\n",
       "100   100.000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coffee_macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "13de61b7-582e-47b9-be93-52e20518bf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Coffee_Research_result.csv',  'w', encoding = 'utf = 8') as file:\n",
    "    \n",
    "     coffee_accuracy.to_csv(file)\n",
    "\n",
    "with open('Coffee_MacroF1_result.csv',  'w', encoding = 'utf = 8') as file:\n",
    "    \n",
    "     coffee_macro_f1.to_csv(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0037eefa-bd7b-4bff-9e17-68842e4bd085",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### <font color = green >The classification Accuracy and Macro-F1 score of the Ham dataset</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "43cc8b81-e14f-4ec1-badd-af0d7b78cfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "Best parameters: {'bc__max_features': 0.8, 'bc__max_samples': 0.8, 'bc__n_estimators': 10, 'bc__random_state': 45}\n",
      "Best cross-validation accuracy: 0.5878378378378378\n",
      "bc Test accuracy: 0.6952380952380952\n",
      "bc Test Macro-F1: 0.695210449927431\n",
      "window size  5  execution completed !\n",
      "\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "Best parameters: {'bc__max_features': 0.8, 'bc__max_samples': 1.0, 'bc__n_estimators': 50, 'bc__random_state': 45}\n",
      "Best cross-validation accuracy: 0.6331331331331331\n",
      "bc Test accuracy: 0.7333333333333333\n",
      "bc Test Macro-F1: 0.7327272727272728\n",
      "window size  10  execution completed !\n",
      "\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "Best parameters: {'bc__max_features': 0.8, 'bc__max_samples': 1.0, 'bc__n_estimators': 20, 'bc__random_state': 45}\n",
      "Best cross-validation accuracy: 0.6053553553553553\n",
      "bc Test accuracy: 0.8\n",
      "bc Test Macro-F1: 0.7993447993447994\n",
      "window size  45  execution completed !\n",
      "\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "Best parameters: {'bc__max_features': 0.8, 'bc__max_samples': 0.8, 'bc__n_estimators': 10, 'bc__random_state': 45}\n",
      "Best cross-validation accuracy: 0.6418918918918919\n",
      "bc Test accuracy: 0.7428571428571429\n",
      "bc Test Macro-F1: 0.7424834226541921\n",
      "window size  50  execution completed !\n",
      "\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "Best parameters: {'bc__max_features': 0.8, 'bc__max_samples': 0.6, 'bc__n_estimators': 20, 'bc__random_state': 45}\n",
      "Best cross-validation accuracy: 0.5227727727727728\n",
      "bc Test accuracy: 0.6476190476190476\n",
      "bc Test Macro-F1: 0.6474911532528809\n",
      "window size  95  execution completed !\n",
      "\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "Best parameters: {'bc__max_features': 0.8, 'bc__max_samples': 0.6, 'bc__n_estimators': 20, 'bc__random_state': 45}\n",
      "Best cross-validation accuracy: 0.504004004004004\n",
      "bc Test accuracy: 0.6666666666666666\n",
      "bc Test Macro-F1: 0.6666666666666666\n",
      "window size  100  execution completed !\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>69.524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>73.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>80.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>74.286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>64.762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>66.667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Accuracy\n",
       "5      69.524\n",
       "10     73.333\n",
       "45     80.000\n",
       "50     74.286\n",
       "95     64.762\n",
       "100    66.667"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ham\n",
    "\n",
    "ham_accuracy, ham_macro_f1 = ham_experiment(embedding_dimension = 2)\n",
    "\n",
    "ham_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "93083ee9-ce1b-4b0f-aab5-6a9aa4ee5fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Macro-F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>69.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>73.273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>79.934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>74.248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>64.749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>66.667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Macro-F1\n",
       "5      69.521\n",
       "10     73.273\n",
       "45     79.934\n",
       "50     74.248\n",
       "95     64.749\n",
       "100    66.667"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c2ad1780-08c4-4fbb-8199-e39126a6a74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Ham_Research_result.csv',  'w', encoding = 'utf = 8') as file:\n",
    "    \n",
    "     ham_accuracy.to_csv(file)\n",
    "\n",
    "with open('Ham_MacroF1_result.csv',  'w', encoding = 'utf = 8') as file:\n",
    "    \n",
    "     ham_macro_f1.to_csv(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8036811d-b747-45ff-960b-f5e389c48e8e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### <font color = green >The classification Accuracy and Macro-F1 score of the Meat dataset</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b8e20096-ddd6-48f8-9656-f51c09f36002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "Best parameters: {'bc__max_features': 1.0, 'bc__max_samples': 1.0, 'bc__n_estimators': 10, 'bc__random_state': 42}\n",
      "Best cross-validation accuracy: 0.9833333333333334\n",
      "bc Test accuracy: 0.95\n",
      "bc Test Macro-F1: 0.9499358151476253\n",
      "window size  5  execution completed !\n",
      "\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "Best parameters: {'bc__max_features': 1.0, 'bc__max_samples': 1.0, 'bc__n_estimators': 10, 'bc__random_state': 42}\n",
      "Best cross-validation accuracy: 1.0\n",
      "bc Test accuracy: 0.95\n",
      "bc Test Macro-F1: 0.9499358151476253\n",
      "window size  10  execution completed !\n",
      "\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "Best parameters: {'bc__max_features': 1.0, 'bc__max_samples': 0.8, 'bc__n_estimators': 10, 'bc__random_state': 42}\n",
      "Best cross-validation accuracy: 1.0\n",
      "bc Test accuracy: 0.95\n",
      "bc Test Macro-F1: 0.9499389499389498\n",
      "window size  45  execution completed !\n",
      "\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "Best parameters: {'bc__max_features': 1.0, 'bc__max_samples': 0.8, 'bc__n_estimators': 20, 'bc__random_state': 42}\n",
      "Best cross-validation accuracy: 1.0\n",
      "bc Test accuracy: 0.95\n",
      "bc Test Macro-F1: 0.9499358151476253\n",
      "window size  50  execution completed !\n",
      "\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "Best parameters: {'bc__max_features': 1.0, 'bc__max_samples': 1.0, 'bc__n_estimators': 10, 'bc__random_state': 42}\n",
      "Best cross-validation accuracy: 0.9833333333333334\n",
      "bc Test accuracy: 0.9833333333333333\n",
      "bc Test Macro-F1: 0.9833229101521784\n",
      "window size  95  execution completed !\n",
      "\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "Best parameters: {'bc__max_features': 1.0, 'bc__max_samples': 1.0, 'bc__n_estimators': 10, 'bc__random_state': 42}\n",
      "Best cross-validation accuracy: 0.9833333333333334\n",
      "bc Test accuracy: 1.0\n",
      "bc Test Macro-F1: 1.0\n",
      "window size  100  execution completed !\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>95.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>95.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>95.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>95.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>98.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>100.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Accuracy\n",
       "5      95.000\n",
       "10     95.000\n",
       "45     95.000\n",
       "50     95.000\n",
       "95     98.333\n",
       "100   100.000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# meat\n",
    "\n",
    "meat_accuracy, meat_macro_f1 = meat_experiment(embedding_dimension = 2)\n",
    "\n",
    "meat_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "71fbf7af-3206-4a39-8ea0-fe96962d400d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Macro-F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>94.994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>94.994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>94.994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>94.994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>98.332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>100.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Macro-F1\n",
       "5      94.994\n",
       "10     94.994\n",
       "45     94.994\n",
       "50     94.994\n",
       "95     98.332\n",
       "100   100.000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meat_macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "342cc80c-d1b5-4b7c-8eee-428649000e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Meat_Research_result.csv',  'w', encoding = 'utf = 8') as file:\n",
    "    \n",
    "     meat_accuracy.to_csv(file)\n",
    "\n",
    "with open('Meat_MacroF1_result.csv',  'w', encoding = 'utf = 8') as file:\n",
    "    \n",
    "     meat_macro_f1.to_csv(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0738b23a-8a99-4251-8169-1c6a15e526e5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### <font color = green >The classification Accuracy and Macro-F1 score of the Olive oil dataset</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1db5f3af-185a-43eb-a7fb-2961bd2a5f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'bc__max_features': 1.0, 'bc__max_samples': 1.0, 'bc__n_estimators': 50, 'bc__random_state': 14}\n",
      "Best cross-validation accuracy: 0.7666666666666666\n",
      "bc Test accuracy: 0.9\n",
      "bc Test Macro-F1: 0.9065079365079365\n",
      "window size  5  execution completed !\n",
      "\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'bc__max_features': 1.0, 'bc__max_samples': 0.6, 'bc__n_estimators': 10, 'bc__random_state': 14}\n",
      "Best cross-validation accuracy: 0.8333333333333334\n",
      "bc Test accuracy: 0.9666666666666667\n",
      "bc Test Macro-F1: 0.9759725400457666\n",
      "window size  10  execution completed !\n",
      "\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "Best parameters: {'bc__max_features': 1.0, 'bc__max_samples': 1.0, 'bc__n_estimators': 100, 'bc__random_state': 14}\n",
      "Best cross-validation accuracy: 0.9\n",
      "bc Test accuracy: 0.9333333333333333\n",
      "bc Test Macro-F1: 0.9513888888888888\n",
      "window size  45  execution completed !\n",
      "\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'bc__max_features': 1.0, 'bc__max_samples': 0.6, 'bc__n_estimators': 30, 'bc__random_state': 14}\n",
      "Best cross-validation accuracy: 0.9\n",
      "bc Test accuracy: 0.9666666666666667\n",
      "bc Test Macro-F1: 0.9759725400457666\n",
      "window size  50  execution completed !\n",
      "\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "Best parameters: {'bc__max_features': 0.8, 'bc__max_samples': 1.0, 'bc__n_estimators': 10, 'bc__random_state': 14}\n",
      "Best cross-validation accuracy: 0.9\n",
      "bc Test accuracy: 0.9666666666666667\n",
      "bc Test Macro-F1: 0.9759725400457666\n",
      "window size  95  execution completed !\n",
      "\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "Best parameters: {'bc__max_features': 1.0, 'bc__max_samples': 1.0, 'bc__n_estimators': 10, 'bc__random_state': 14}\n",
      "Best cross-validation accuracy: 0.8666666666666667\n",
      "bc Test accuracy: 0.9666666666666667\n",
      "bc Test Macro-F1: 0.9759725400457666\n",
      "window size  100  execution completed !\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>90.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>96.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>93.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>96.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>96.667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Accuracy\n",
       "5      90.000\n",
       "10     96.667\n",
       "45     93.333\n",
       "50     96.667\n",
       "95     96.667\n",
       "100    96.667"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oliveoil\n",
    "\n",
    "oliveoil_accuracy, oliveoil_macro_f1 = oliveoil_experiment(embedding_dimension = 2)\n",
    "\n",
    "oliveoil_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c4d55d0c-ce30-4d1e-8e7c-13a1dc4eaf5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Macro-F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>90.651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>97.597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>95.139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>97.597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>97.597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>97.597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Macro-F1\n",
       "5      90.651\n",
       "10     97.597\n",
       "45     95.139\n",
       "50     97.597\n",
       "95     97.597\n",
       "100    97.597"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oliveoil_macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4c2e13eb-bea4-45d2-bc36-aada48182ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Oliveoil_Research_result.csv',  'w', encoding = 'utf = 8') as file:\n",
    "    \n",
    "     oliveoil_accuracy.to_csv(file)\n",
    "\n",
    "with open('Oliveoil_MacroF1_result.csv',  'w', encoding = 'utf = 8') as file:\n",
    "    \n",
    "     oliveoil_macro_f1.to_csv(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d2c89e-25df-4da7-853a-669755989ec8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### <font color = green >The classification Accuracy and Macro-F1 score of the Strawberry dataset</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ae9bd8ef-a622-41da-b56e-e69090838d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'bc__max_features': 1.0, 'bc__max_samples': 1.0, 'bc__n_estimators': 20, 'bc__random_state': 14}\n",
      "Best cross-validation accuracy: 0.9509963334927467\n",
      "bc Test accuracy: 0.9783783783783784\n",
      "bc Test Macro-F1: 0.9764451235039471\n",
      "window size  5  execution completed !\n",
      "\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'bc__max_features': 1.0, 'bc__max_samples': 1.0, 'bc__n_estimators': 20, 'bc__random_state': 14}\n",
      "Best cross-validation accuracy: 0.9412083532600032\n",
      "bc Test accuracy: 0.9567567567567568\n",
      "bc Test Macro-F1: 0.9532001011889704\n",
      "window size  10  execution completed !\n",
      "\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'bc__max_features': 1.0, 'bc__max_samples': 0.8, 'bc__n_estimators': 30, 'bc__random_state': 14}\n",
      "Best cross-validation accuracy: 0.9248923959827833\n",
      "bc Test accuracy: 0.9432432432432433\n",
      "bc Test Macro-F1: 0.9390545214956351\n",
      "window size  45  execution completed !\n",
      "\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'bc__max_features': 1.0, 'bc__max_samples': 0.6, 'bc__n_estimators': 20, 'bc__random_state': 14}\n",
      "Best cross-validation accuracy: 0.9200063765343535\n",
      "bc Test accuracy: 0.927027027027027\n",
      "bc Test Macro-F1: 0.9218768328445748\n",
      "window size  50  execution completed !\n",
      "\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "Best parameters: {'bc__max_features': 1.0, 'bc__max_samples': 1.0, 'bc__n_estimators': 50, 'bc__random_state': 14}\n",
      "Best cross-validation accuracy: 0.8955364259524948\n",
      "bc Test accuracy: 0.9108108108108108\n",
      "bc Test Macro-F1: 0.9060848697417911\n",
      "window size  95  execution completed !\n",
      "\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "Best parameters: {'bc__max_features': 1.0, 'bc__max_samples': 0.8, 'bc__n_estimators': 50, 'bc__random_state': 14}\n",
      "Best cross-validation accuracy: 0.8971544715447154\n",
      "bc Test accuracy: 0.9081081081081082\n",
      "bc Test Macro-F1: 0.9036015325670498\n",
      "window size  100  execution completed !\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>97.838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>95.676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>94.324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>92.703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>91.081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>90.811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Accuracy\n",
       "5      97.838\n",
       "10     95.676\n",
       "45     94.324\n",
       "50     92.703\n",
       "95     91.081\n",
       "100    90.811"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# strawberry\n",
    "\n",
    "strawberry_accuracy, strawberry_macro_f1 = strawberry_experiment(embedding_dimension = 2)\n",
    "\n",
    "strawberry_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e73fcfdf-d3e8-4d7b-b251-22122720d644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Macro-F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>97.645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>95.320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>93.905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>92.188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>90.608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>90.360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Macro-F1\n",
       "5      97.645\n",
       "10     95.320\n",
       "45     93.905\n",
       "50     92.188\n",
       "95     90.608\n",
       "100    90.360"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strawberry_macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0681b75b-5910-4b03-ad2e-07874677fa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Strawberry_Research_result.csv',  'w', encoding = 'utf = 8') as file:\n",
    "    \n",
    "     strawberry_accuracy.to_csv(file)\n",
    "\n",
    "with open('Strawberry_MacroF1_result.csv',  'w', encoding = 'utf = 8') as file:\n",
    "    \n",
    "     strawberry_macro_f1.to_csv(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1535b9c-8ad2-4bac-baac-132bb6a9e02c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### <font color = green >The classification Accuracy and Macro-F1 score of the Wine dataset</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "34761216-b4d9-41d1-aeb2-7c26291783ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "Best parameters: {'bc__max_samples': 0.6, 'bc__n_estimators': 30, 'bc__random_state': 3}\n",
      "Best cross-validation accuracy: 0.7017543859649122\n",
      "bc Test accuracy: 0.8333333333333334\n",
      "bc Test Macro-F1: 0.8328173374613004\n",
      "window size  5  execution completed !\n",
      "\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "Best parameters: {'bc__max_samples': 1.0, 'bc__n_estimators': 30, 'bc__random_state': 3}\n",
      "Best cross-validation accuracy: 0.7894736842105264\n",
      "bc Test accuracy: 0.9259259259259259\n",
      "bc Test Macro-F1: 0.9258241758241759\n",
      "window size  10  execution completed !\n",
      "\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "Best parameters: {'bc__max_samples': 0.8, 'bc__n_estimators': 10, 'bc__random_state': 3}\n",
      "Best cross-validation accuracy: 0.6140350877192983\n",
      "bc Test accuracy: 0.6481481481481481\n",
      "bc Test Macro-F1: 0.6480274442538594\n",
      "window size  45  execution completed !\n",
      "\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "Best parameters: {'bc__max_samples': 1.0, 'bc__n_estimators': 100, 'bc__random_state': 3}\n",
      "Best cross-validation accuracy: 0.6842105263157895\n",
      "bc Test accuracy: 0.7037037037037037\n",
      "bc Test Macro-F1: 0.7032967032967032\n",
      "window size  50  execution completed !\n",
      "\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "Best parameters: {'bc__max_samples': 1.0, 'bc__n_estimators': 30, 'bc__random_state': 3}\n",
      "Best cross-validation accuracy: 0.6842105263157894\n",
      "bc Test accuracy: 0.8333333333333334\n",
      "bc Test Macro-F1: 0.8328173374613004\n",
      "window size  95  execution completed !\n",
      "\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "Best parameters: {'bc__max_samples': 1.0, 'bc__n_estimators': 30, 'bc__random_state': 3}\n",
      "Best cross-validation accuracy: 0.6666666666666666\n",
      "bc Test accuracy: 0.7962962962962963\n",
      "bc Test Macro-F1: 0.7962264150943396\n",
      "window size  100  execution completed !\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>83.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>92.593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>64.815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>70.370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>83.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>79.630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Accuracy\n",
       "5      83.333\n",
       "10     92.593\n",
       "45     64.815\n",
       "50     70.370\n",
       "95     83.333\n",
       "100    79.630"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wine\n",
    "\n",
    "wine_accuracy, wine_macro_f1 = wine_experiment(embedding_dimension = 2)\n",
    "\n",
    "wine_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c3f41a76-5f1d-4dca-ab05-101f31efa21c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Macro-F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>83.282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>92.582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>64.803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>70.330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>83.282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>79.623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Macro-F1\n",
       "5      83.282\n",
       "10     92.582\n",
       "45     64.803\n",
       "50     70.330\n",
       "95     83.282\n",
       "100    79.623"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1a8c4ffa-6500-481f-b05e-0dbeb1cb577c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Wine_Research_result.csv',  'w', encoding = 'utf = 8') as file:\n",
    "    \n",
    "     wine_accuracy.to_csv(file)\n",
    "\n",
    "with open('Wine_MacroF1_result.csv',  'w', encoding = 'utf = 8') as file:\n",
    "    \n",
    "     wine_macro_f1.to_csv(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
